#!/usr/bin/env python
import os

# Set logger level.
if config["settings_extra"]["debug"]:
    logger.set_level("DEBUG")

# Validate input.
if sum([x is not None for x in [config["inputs"]["genelimit"], config["inputs"]["snplimit"], config["inputs"]["snpgenelimit"]]]) > 1:
    logger.error("Critical, selected multiple limitation files. Use genelimit, snplimit, snpgenelimit or none.\n\nExiting.")
    exit("InvalidInput")
if config["general_settings"]["n_genes"] is not None and config["general_settings"]["n_genes"] <= 0:
    logger.error("Critical, n_genes needs to be larger than zero.\n\nExiting.")
    exit("InvalidInput")

# Check required input arguments.
if config["inputs"]["singularity_image"] is None or not os.path.exists(config["inputs"]["singularity_image"]):
    logger.error("Critical, singularity_image does not exist.\n\nExiting.")
    exit("MissingsingularityImage")
if config["inputs"]["repo_dir"] is None or not os.path.exists(config["inputs"]["repo_dir"]):
    logger.error("Critical, repo_dir does not exist.\n\nExiting.")
    exit("MissingRepoDir")
if config["inputs"]["mbqtl_jar"] is None or not os.path.exists(config["inputs"]["mbqtl_jar"]):
    logger.error("Critical, mbqtl_jar does not exist.\n\nExiting.")
    exit("MissingsingmbQTLJar")
if config["inputs"]["annotation"] is None or not os.path.exists(config["inputs"]["annotation"]):
    logger.error("Critical, annotation does not exist.\n\nExiting.")
    exit("MissingsingAnnotation")
if config["inputs"]["vcf"] is None or not os.path.exists(config["inputs"]["vcf"]):
    logger.error("Critical, vcf does not exist.\n\nExiting.")
    exit("MissingsingVCF")
if config["inputs"]["exp"] is None or not os.path.exists(config["inputs"]["exp"]):
    logger.error("Critical, exp does not exist.\n\nExiting.")
    exit("MissingsingExp")
if config["inputs"]["gte"] is None or not os.path.exists(config["inputs"]["gte"]):
    logger.error("Critical, gte does not exist.\n\nExiting.")
    exit("MissingsingGTE")
if config["outputs"]["output_dir"] is None:
    logger.error("Critical, the output_dir cannot be empty.\n\nExiting.")
    exit("MissingOutputDir")

# Check optional input arguments.
if config["inputs"]["cov"] is not None and not os.path.exists(config["inputs"]["cov"]):
    logger.error("Critical, cov does not exist.\n\nExiting.")
    exit("InvalidCov")
if config["inputs"]["genelimit"] is not None and not os.path.exists(config["inputs"]["genelimit"]):
    logger.error("Critical, genelimit does not exist.\n\nExiting.")
    exit("InvalidGeneLimit")
if config["inputs"]["snplimit"] is not None and not os.path.exists(config["inputs"]["snplimit"]):
    logger.error("Critical, snplimit does not exist.\n\nExiting.")
    exit("InvalidSNPLimit")
if config["inputs"]["snpgenelimit"] is not None and not os.path.exists(config["inputs"]["snpgenelimit"]):
    logger.error("Critical, snpgenelimit does not exist.\n\nExiting.")
    exit("InvalidSNPGeneLimit")

if config["inputs"]["genelimit"] is not None and not os.path.exists(config["inputs"]["genelimit"]):
    logger.error("Critical, the genelimit file does not exist.\n\nExiting.")
    exit("MissingGeneLimitFile")
if config["inputs"]["snpgenelimit"] is not None and not os.path.exists(config["inputs"]["snpgenelimit"]):
    logger.error("Critical, the snpgenelimit file does not exist.\n\nExiting.")
    exit("MissingSNPGeneLimitFile")

# Checking settings with options to be valid.
if config["general_settings"]["force_mega"] not in ["all", "cov", "qtl", "none", None]:
    logger.error("Critical, force_mega must be 'all', 'cov', 'qtl', 'none', or empty.\n\nExiting.")
    exit("InvalidForceMegaSetting")
if config["qtl_settings"]["analysis_type"] not in ["cis", "trans", "cistrans"]:
    logger.error("Critical, analysis_type must be 'cis', 'trans', or 'cistrans'.\n\nExiting.")
    exit("InvalidAnalysisType")
if config["qtl_settings"]["meta_analysis_method"] not in ["empzmeta", "fisherzmeta", "fisherzmetarandom"]:
    logger.error("Critical, meta_analysis_method must be 'empzmeta', 'fisherzmeta', or 'fisherzmetarandom'.\n\nExiting.")
    exit("InvalidMetaAnalysisMethod")

# Replace non required variables that are None with empty strings.
for input_variable in ["cov", "genelimit", "snplimit", "snpgenelimit"]:
    if config["inputs"][input_variable] is None:
        config["inputs"][input_variable] = ""

# Set the default output prefix.
if config["outputs"]["output_prefix"] is None:
    config["outputs"]["output_prefix"] = "mbQTL"

# Reformat the selected number of PCs into a list and remove 0 Pcs.
if config["general_settings"]["n_pcs"] is None:
    config["general_settings"]["n_pcs"] = []
if not isinstance(config["general_settings"]["n_pcs"], list):
    config["general_settings"]["n_pcs"] = [config["general_settings"]["n_pcs"]]
if 0 in config["general_settings"]["n_pcs"]:
    config["general_settings"]["n_pcs"].remove(0)

# Reformat force mega.
if config["general_settings"]["force_mega"] == "all":
    config["general_settings"]["force_mega"] = ["cov", "qtl"]
else:
    config["general_settings"]["force_mega"] = [config["general_settings"]["force_mega"]]

# Add trailing /.
if not config["inputs"]["repo_dir"].endswith("/"):
    config["inputs"]["repo_dir"] += "/"
if not config["outputs"]["output_dir"].endswith("/"):
    config["outputs"]["output_dir"] += "/"

# Check setting combinations that do not make sense and update accordingly.
if config["settings_extra"]["force"]:
    logger.warning("Warning, unlogical input arguments will not be automatically updated. Use with caution!")
else:
    if (config["inputs"]["snpgenelimit"] != "" or config["inputs"]["snplimit"] != "") and config["general_settings"]["use_snpannotation"]:
        logger.warning("Warning, setting use_snpannotation to False since snpgenelimit / snplimit is used.")
        config["general_settings"]["use_snpannotation"] = False
    if config["inputs"]["snpgenelimit"] == "" and config["inputs"]["snplimit"] == "" and config["general_settings"]["filter_vcf"]:
        logger.warning("Warning, setting filter_vcf to False since snpgenelimit / snplimit is not used.")
        config["general_settings"]["filter_vcf"] = False
    if config["inputs"]["snpgenelimit"] != "" and not config["qtl_settings"]["outputall"]:
        logger.warning("Warning, setting outputall to True since snpgenelimit is used.")
        config["qtl_settings"]["outputall"] = True
    if config["inputs"]["snpgenelimit"] != "" and not config["qtl_settings"]["perm"] == 0:
        logger.warning("Warning, setting perm to 0 since snpgenelimit is used.")
        config["qtl_settings"]["perm"] = 0
    if "qtl" in config["general_settings"]["force_mega"] and config["qtl_settings"]["nrdatasets"] > 1:
        logger.warning("Warning, setting nrdatasets to 1 since force_mega for QTL mapping is True.")
        config["qtl_settings"]["nrdatasets"] = 1

# Check setting combinations that are not allowed.
if config["qtl_settings"]["nrdatasets"] > 1 and config["inputs"]["gte"].endswith(".smf"):
    logger.error("Critical, setting the minimum number of datasets required in "
                    "meta-analysis (nrdatasets) > 1 will result in no results "
                    "since your input GTE has no dataset column.")
    exit()
if config["qtl_settings"]["replacemissinggenotypes"] and config["qtl_settings"]["perm"] == 0:
    logger.error("Critical, replacemissinggenotypes is True while perm is 0. This setting should "
                    "only be True when both genotypes and expression data have missing "
                    "values and perm > 0.\n\nExiting.")
    exit()

# Some friendly reminders.
if config["qtl_settings"]["nrdatasets"] > 1:
    logger.error("Error, setting the minimum number of datasets required in "
                  "meta-analysis (nrdatasets) > 1 will result in no results "
                  "if your input GTE only contains one dataset.")
if config["inputs"]["snpgenelimit"] != "" and config["run_qtl_time"] != 0:
    logger.warning("Warning, you are only testing a select number of effects but the runtime for each "
                   "job is max {}. If you tink this is too long you should decrease "
                   "run_qtl_time to e.g. 0.".format(config["cluster_time"][config["run_qtl_time"]]))
if config["inputs"]["snpgenelimit"] == "" and config["general_settings"]["n_genes"] is None and config["run_qtl_time"] == 0:
    logger.warning("Warning, you are running all tests in one job file but the runtime for this "
                   "job is max {}. If you tink this is too short you should increase "
                   "run_qtl_time to 1 or higher.".format(config["cluster_time"][config["run_qtl_time"]]))
if config["inputs"]["snpgenelimit"] == "" and config["general_settings"]["n_genes"] is not None and config["run_qtl_time"] != 0:
    logger.warning("Warning, you are running tests in multiple job files but the runtime for each "
                   "job is max {}. If you tink this is too long you should decrease "
                   "run_qtl_time to e.g. 0.".format(config["cluster_time"][config["run_qtl_time"]]))

# Automatically determine the mode of the input.
input_mode = "default"
if config["inputs"]["cov"] != "" and not config["general_settings"]["n_pcs"]:
    input_mode = "cov"
elif config["inputs"]["cov"] != "" and config["general_settings"]["n_pcs"]:
    input_mode = "covXPcs"
elif config["inputs"]["cov"] == "" and config["general_settings"]["n_pcs"]:
    input_mode = "XPcs"
logger.info("Your input is in mode: {}.".format(input_mode))

# Define the tests that the user wants to run.
modes = ["all", "default", "cov", "covXPcs", "XPcs"]
selected_modes = set()
if config["general_settings"]["include_modes"] is None:
    # No user input defaults to the input mode.
    config["general_settings"]["include_modes"] = input_mode
if not isinstance(config["general_settings"]["include_modes"], list):
    # Force input into a list.
    config["general_settings"]["include_modes"] = [config["general_settings"]["include_modes"]]
for mode in config["general_settings"]["include_modes"]:
    # Validate that each mode is known.
    if mode not in modes:
        logger.error("Critical, unexpected mode '{}'.\n\nExiting.".format(mode))
        exit()
# Construct the selected modes.
if "all" in config["general_settings"]["include_modes"]:
    selected_modes = set(modes)
else:
    selected_modes = set(config["general_settings"]["include_modes"])
logger.info("You selected input to execute the following modes: {}.".format(", ".join(selected_modes)))

# Combine the output modes (henceforth named covariates).
covariates = []
if "default" in selected_modes:
    covariates.append("default")
if "cov" in selected_modes:
    if config["inputs"]["cov"] == "":
        logger.warning("Warning, you requested to run mode 'cov' but the 'cov' input is empty. Skipping mode.")
    else:
        covariates.append("cov")
if "XPcs" in selected_modes:
    if not config["general_settings"]["n_pcs"]:
        logger.warning("Warning, you requested to run mode 'XPcs' but the 'n_pcs' input is empty. Skipping mode.")
    else:
        for n_pcs in config["general_settings"]["n_pcs"]:
            covariates.append(str(n_pcs) + "Pcs")
if "covXPcs" in selected_modes:
    if config["inputs"]["cov"] == "":
        logger.warning("Warning, you requested to run mode 'covXPcs' but the 'cov' input is empty. Skipping mode.")
    elif not config["general_settings"]["n_pcs"]:
        logger.warning("Warning, you requested to run mode 'covXPcs' but the 'n_pcs' input is empty. Skipping mode.")
    else:
        for n_pcs in config["general_settings"]["n_pcs"]:
            covariates.append("cov" + str(n_pcs) + "Pcs")
if len(covariates) > 0:
    logger.info("Generating output for the following modes: {}.".format(", ".join(covariates)))


wildcard_constraints:
    n_pcs = "[0-9]+"


def get_basename(fpath):
    return os.path.basename(fpath).rstrip(".gz").rstrip(".txt").rstrip(".tsv").rstrip(".csv").rstrip(".vcf")


def print_wildcards(wildcards):
    out = []
    for key, value in wildcards.items():
        out.append(key + "=" + value)
    return ", ".join(out)


def get_input(wildcards):
    logger.debug("get_input(" + print_wildcards(wildcards) + ")")
    input = []

    if config["general_settings"]["plot_pca"]:
        logger.debug("\trequesting output of rule 'pca' on the input 'exp'")
        input.append(config["outputs"]["output_dir"] + "pca/raw/" + get_basename(config["inputs"]["exp"]) + ".done")

        if "cov" in config["general_settings"]["force_mega"]:
            logger.debug("\trequesting output of rule 'pca' on the output of rule 'regressor'")
            input.extend(expand(config["outputs"]["output_dir"] + "pca/all/" + get_basename(config["inputs"]["exp"]) + ".{cov}.CovariatesRemovedOLS.done", cov=covariates))
        else:
            if "cov" in covariates:
                logger.debug("\trequesting output of rule 'pca' on the output of rule 'regressor'")
                input.append(config["outputs"]["output_dir"] + "pca/all/" + get_basename(config["inputs"]["exp"]) + ".cov.CovariatesRemovedOLS.done")

            ds_corr_covariates = [cov for cov in covariates if cov not in ["default", "cov", "0Pcs", "cov0Pcs"]]
            if len(ds_corr_covariates) > 0 and not "cov" in config["general_settings"]["force_mega"]:
                datasets = get_datasets(wildcards)

                logger.debug("\trequesting output of rule 'pca' on the output of rule 'filter_exp' for a specific dataset")
                input.extend(expand(config["outputs"]["output_dir"] + "pca/{dataset}/" + get_basename(config["inputs"]["exp"]) + ".done", dataset=datasets))

                logger.debug("\trequesting output of rule 'pca' on the output of rule 'regressor' for a specific dataset")
                input.extend(expand(config["outputs"]["output_dir"] + "pca/{dataset}/" + get_basename(config["inputs"]["exp"]) + ".{cov}.CovariatesRemovedOLS.done", dataset=datasets, cov=ds_corr_covariates))

                logger.debug("\trequesting output of rule 'pca' on the output of rule 'merge_exps'")
                input.extend(expand(config["outputs"]["output_dir"] + "pca/merged/" + get_basename(config["inputs"]["exp"]) + ".{cov}.CovariatesRemovedOLS.done", cov=ds_corr_covariates))

    # Output of mbQTL.
    logger.debug("\trequesting output of rule 'run_qtl'")
    input.extend(expand(config["outputs"]["output_dir"] + "output/{cov}/" + config["outputs"]["output_prefix"] + "-TopEffectsWithqval.txt", cov=covariates))

    # Output of results.
    if covariates:
        logger.debug("\trequesting output of rule 'results'")
        input.append(config["outputs"]["output_dir"] + "output/" + config["outputs"]["output_prefix"] + "-results.txt")

    return input


def get_gte(wildcards):
    logger.debug("get_gte(" + print_wildcards(wildcards) + ")")
    if config["inputs"]["gte"].endswith(".smf"):
        logger.debug("\treturn output of rule 'smf_to_gte'")
        return config["outputs"]["output_dir"] + "gte/gte.txt"

    logger.debug("\treturn input 'gte'")
    return config["inputs"]["gte"]


def get_variants(wildcards):
    logger.debug("get_variants(" + print_wildcards(wildcards) + ")")
    if config["inputs"]["snplimit"] == "" and config["inputs"]["snpgenelimit"] != "":
        logger.debug("\treturn output of rule 'snpgenelimit_to_vars'")
        return config["outputs"]["output_dir"] + "genotype/variants.txt"
    elif config["inputs"]["snplimit"] != "" and config["inputs"]["snpgenelimit"] == "":
        logger.debug("\treturn input 'snplimit'")
        return config["inputs"]["snplimit"]
    else:
        logger.error("Error in get_variants")
        exit()

    return None

def get_dataset_gte(wildcards):
    logger.debug("get_dataset_gte(" + print_wildcards(wildcards) + ")")
    if wildcards.dataset in ["raw", "merged", "all"]:
        logger.debug("\tcalling 'get_gte' function")
        return get_gte(wildcards)

    logger.debug("\treturn output of checkpoint 'split_gte'")
    return config["outputs"]["output_dir"] + "gte/split/gte_" + wildcards.dataset + ".txt"


def get_pca_expr(wildcards):
    raw_infile = wildcards.infile == get_basename(config["inputs"]["exp"])
    logger.debug("get_pca_expr(" + print_wildcards(wildcards) + ")")

    if raw_infile:
        # i.e. no covariate correction is needed. Only need to pick between the
        # full input matrix or a subset of the input matrix.
        if wildcards.dataset == "raw" or "cov" in config["general_settings"]["force_mega"]:
            logger.debug("\treturning input 'exp'")
            return config["inputs"]["exp"]

        logger.debug("\treturning output of rule 'filter_exp' for a specific dataset")
        return config["outputs"]["output_dir"] + "exp/" + wildcards.dataset + "/" + wildcards.infile + ".txt.gz"

    # covariate corrected per dataset merged into one file.
    elif wildcards.dataset == "merged":
        logger.debug("\treturning output of rule 'merge_exps'")
        return config["outputs"]["output_dir"] + "regressor/merged." + wildcards.infile + ".txt.gz"

    # else, covariate corrected per dataset.
    logger.debug("\treturning output of rule 'regressor'")
    return config["outputs"]["output_dir"] + "regressor/" + wildcards.dataset + "/" + wildcards.infile + ".txt.gz"

def get_regressor_exp(wildcards):
    logger.debug("get_regressor_exp(" + print_wildcards(wildcards) + ")")
    if wildcards.dataset == "all":
        logger.debug("\treturning input 'exp'")
        return config["inputs"]["exp"]

    logger.debug("\treturning output of rule 'filter_exp' for a specific dataset")
    return config["outputs"]["output_dir"] + "exp/" + wildcards.dataset + "/" + get_basename(config["inputs"]["exp"]) + ".txt.gz"


def get_cov(wildcards):
    logger.debug("get_cov(" + print_wildcards(wildcards) + ")")
    if wildcards.cov == "cov" or wildcards.cov == "cov0Pcs":
        logger.debug("\treturning input 'cov'")
        return config["inputs"]["cov"]
    elif not wildcards.cov.startswith("cov") and wildcards.cov.endswith("Pcs"):
        logger.debug("\treturning output of rule 'pca' for a specific dataset")
        return config["outputs"]["output_dir"] + "pca/{dataset}/" + get_basename(config["inputs"]["exp"]) + ".{cov}.txt.gz"

    logger.debug("\treturning output of rule 'merge_covs' for a specific dataset")
    return config["outputs"]["output_dir"] + "cov/{dataset}/" + get_basename(config["inputs"]["exp"]) + ".{cov}.txt.gz"


def get_datasets(wildcards):
    logger.debug("get_datasets(" + print_wildcards(wildcards) + ")")
    out_dir = checkpoints.split_gte.get(**wildcards).output[0]
    datasets = glob_wildcards(os.path.join(out_dir, "gte_{dataset}.txt"))
    logger.debug("\tfound datasets: '" + ", ".join(datasets.dataset) + "'")

    return datasets.dataset


def get_exp(wildcards):
    logger.debug("get_exp(" + print_wildcards(wildcards) + ")")
    if wildcards.cov == "default" or wildcards.cov == "0Pcs":
        logger.debug("\treturning input 'exp'")
        return config["inputs"]["exp"]
    elif wildcards.cov == "cov" or "cov" in config["general_settings"]["force_mega"]:
        logger.debug("\treturning output of rule 'regressor'")
        return config["outputs"]["output_dir"] + "regressor/all/" + get_basename(config["inputs"]["exp"]) + ".{cov}.CovariatesRemovedOLS.txt.gz"

    logger.debug("\treturning outout of rule 'merge_exps'")
    return config["outputs"]["output_dir"] + "regressor/merged." + get_basename(config["inputs"]["exp"]) + ".{cov}.CovariatesRemovedOLS.txt.gz"


def get_annotation(wildcards):
    logger.debug("get_annotation(" + print_wildcards(wildcards) + ")")
    if config["inputs"]["annotation"].endswith(".gtf"):
        logger.debug("\treturning output of rule 'create_gene_annotation'")
        return config["outputs"]["output_dir"] + "create_annotation/refdata-gex-GeneAnnotation.txt.gz"

    logger.debug("\treturning input 'annotation'")
    return config["inputs"]["annotation"]


def get_snpannotation(wildcards):
    logger.debug("get_snpannotation(" + print_wildcards(wildcards) + ")")
    if config["general_settings"]["use_snpannotation"]:
        logger.debug("\treturning output of rule 'create_snp_annotation'")
        return config["outputs"]["output_dir"] + "create_annotation/snp_annotation.txt.gz"

    logger.debug("\treturning nothing")
    return []


def get_vcf(wildcards):
    logger.debug("get_vcf(" + print_wildcards(wildcards) + ")")
    if config["inputs"]["snpgenelimit"] != "" and config["general_settings"]["filter_vcf"]:
        logger.debug("\treturning output of rule 'filter_vcf'")
        return config["outputs"]["output_dir"] + "genotype/" + get_basename(config["inputs"]["vcf"]) + "_" + get_basename(config["inputs"]["snpgenelimit"]) + ".vcf.bgz"

    logger.debug("\treturning input 'vcf'")
    return config["inputs"]["vcf"]


def get_vcf_index(wildcards):
    logger.debug("get_vcf_index(" + print_wildcards(wildcards) + ")")
    logger.debug("\tcalling 'get_vcf' function")
    return get_vcf(wildcards) + ".tbi"


def get_qtl_gte(wildcards):
    logger.debug("get_qtl_gte(" + print_wildcards(wildcards) + ")")
    if "qtl" in config["general_settings"]["force_mega"]:
        logger.debug("\treturn output of rule 'remove_gte_dataset'")
        return config["outputs"]["output_dir"] + "gte/gte_noDataset.txt"

    logger.debug("\tcalling 'get_gte' function")
    return get_gte(wildcards)


def get_genelimit(wildcards):
    logger.debug("get_genelimit(" + print_wildcards(wildcards) + ")")
    prefix = "--genelimit "
    if config["inputs"]["genelimit"] != "":
        logger.debug("\treturning input 'genelimit'")
        return prefix + config["inputs"]["genelimit"]
    elif config["general_settings"]["n_genes"] is not None:
        logger.debug("\treturning output of checkpoint 'create_batches'")
        return prefix + config["outputs"]["output_dir"] + "batches/" + wildcards.cov + "/" + wildcards.batch + "-genes.txt"

    logger.debug("\treturning nothing")
    return ""

def get_snplimit(wildcards):
    logger.debug("get_snplimit(" + print_wildcards(wildcards) + ")")
    prefix = "--snplimit "
    if config["inputs"]["snplimit"] != "":
        logger.debug("\treturning input 'snplimit'")
        return prefix + config["inputs"]["snplimit"]

    logger.debug("\treturning nothing")
    return ""

def get_snpgenelimit(wildcards):
    logger.debug("get_snpgenelimit(" + print_wildcards(wildcards) + ")")
    prefix = "--snpgenelimit "
    if config["inputs"]["snpgenelimit"] != "":
        logger.debug("\treturning input 'snpgenelimit'")
        return prefix + config["inputs"]["snpgenelimit"]

    logger.debug("\treturning nothing")
    return ""

def get_batches(wildcards):
    logger.debug("get_batches(" + print_wildcards(wildcards) + ")")
    out_dir = checkpoints.create_batches.get(**wildcards).output[0]
    batches = glob_wildcards(os.path.join(out_dir, "{batch}-genes.txt"))
    logger.debug("\tfound batches: '" + ", ".join(batches.batch) + "'")

    return batches.batch


def get_top_hits(wildcards):
    logger.debug("get_top_hits(" + print_wildcards(wildcards) + ")")
    if config["general_settings"]["n_genes"] is None:
        logger.debug("\treturning output of rule 'run_qtl'")
        return config["outputs"]["output_dir"] + "output/{cov}/" + config["outputs"]["output_prefix"] + "-TopEffects.txt"

    logger.debug("\treturning output of rule 'merge'")
    return config["outputs"]["output_dir"] + "output/{cov}/" + config["outputs"]["output_prefix"] + "-merged.txt"


rule all:
    input: get_input


rule create_gene_annotation:
    input:
        gtf = config["inputs"]["annotation"]
    output:
        dupl_annotation = temp(config["outputs"]["output_dir"] + "create_annotation/refdata-gex-GeneAnnotation.WithDuplicates.txt.gz"),
        annotation = config["outputs"]["output_dir"] + "create_annotation/refdata-gex-GeneAnnotation.txt.gz"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["create_gene_annotation_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["create_gene_annotation_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["create_gene_annotation_time"]]
    threads: config["create_gene_annotation_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/create_annotation_file.py",
        feature_name = config["create_annotation_settings"]["feature_name"],
        autosomes_only = "--autosomes_only " if config["create_annotation_settings"]["autosomes_only"] else "",
        out = config["outputs"]["output_dir"] + "create_annotation/"
    log: config["outputs"]["output_dir"] + "log/create_gene_annotation.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --in_gtf {input.gtf} \
            --feature_name {params.feature_name} \
            {params.autosomes_only} \
            --out_dir {params.out} > {log} 2>&1
        """


rule create_snp_annotation:
    input:
        vcf = config["inputs"]["vcf"]
    output:
        annotation = config["outputs"]["output_dir"] + "create_annotation/snp_annotation.txt.gz"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["create_snp_annotation_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["create_snp_annotation_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["create_snp_annotation_time"]]
    threads: config["create_snp_annotation_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
    log: config["outputs"]["output_dir"] + "log/create_snp_annotation.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} zcat {input.vcf} | grep "^[^#;]" | awk -v OFS='\t' '{{print $3,$1,$2}}' | gzip -c > {output.annotation}
        """


rule smf_to_gte:
    input:
        smf = config["inputs"]["gte"],
    output:
        gte = config["outputs"]["output_dir"] + "gte/gte.txt"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["smf_to_gte_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["smf_to_gte_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["smf_to_gte_time"]]
    threads: config["smf_to_gte_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
    log: config["outputs"]["output_dir"] + "log/smf_to_gte.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} awk 'BEGIN{{ FS = OFS = "\\t" }} {{ print $1, $2, "Dataset" }}' {input.smf} > {output.gte}
        """


checkpoint split_gte:
    priority: 50
    input:
        gte = get_gte,
    output:
        datasets = directory(config["outputs"]["output_dir"] + "gte/split/")
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["split_gte_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["split_gte_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["split_gte_time"]]
    threads: config["split_gte_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/split_gte.py",
        out = config["outputs"]["output_dir"] + "gte/split/"
    log: config["outputs"]["output_dir"] + "log/split_gte.log"
    shell:
        """
        mkdir -p {output.datasets}
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --gte {input.gte} \
            --out {params.out} > {log} 2>&1
        """


rule snpgenelimit_to_vars:
    input:
        sgl = config["inputs"]["snpgenelimit"]
    output:
        vars = temp(config["outputs"]["output_dir"] + "genotype/variants.txt")
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["snpgenelimit_to_vars_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["snpgenelimit_to_vars_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["snpgenelimit_to_vars_time"]]
    threads: config["snpgenelimit_to_vars_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        cat = "zcat" if config["inputs"]["snpgenelimit"].endswith(".gz") else "cat",
    log: config["outputs"]["output_dir"] + "log/snpgenelimit_to_vars.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} {params.cat} {input.sgl} | awk 'BEGIN{{ FS = OFS = "\\t" }} {{ print $1 }}' > {output.vars}
        """


rule index_vcf:
    input:
        vcf = config["inputs"]["vcf"]
    output:
        index = config["inputs"]["vcf"] + ".tbi"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["index_vcf_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["index_vcf_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["index_vcf_time"]]
    threads: config["index_vcf_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
    log: config["outputs"]["output_dir"] + "log/index_vcf.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} tabix -p vcf {input.vcf}
        """


rule filter_vcf:
    input:
        vcf = config["inputs"]["vcf"],
        sgl = config["inputs"]["snpgenelimit"],
        gte = get_gte,
        vars = get_variants
    output:
        samples = temp(config["outputs"]["output_dir"] + "genotype/samples.txt"),
        vcf_gz = temp(config["outputs"]["output_dir"] + "genotype/" + get_basename(config["inputs"]["vcf"]) + "_" + get_basename(config["inputs"]["snpgenelimit"]) + ".vcf.gz"),
        vcf_bgz = config["outputs"]["output_dir"] + "genotype/" + get_basename(config["inputs"]["vcf"]) + "_" + get_basename(config["inputs"]["snpgenelimit"]) + ".vcf.bgz",
        index = config["outputs"]["output_dir"] + "genotype/" + get_basename(config["inputs"]["vcf"]) + "_" + get_basename(config["inputs"]["snpgenelimit"]) + ".vcf.bgz.tbi"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["filter_vcf_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["filter_vcf_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["filter_vcf_time"]]
    threads: config["filter_vcf_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/filter_vcf.py"
    log: config["outputs"]["output_dir"] + "log/filter_vcf.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} awk 'BEGIN{{ FS = OFS = "\\t" }} {{ print $1 }}' {input.gte} > {output.samples}
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --vcf {input.vcf} \
            --variants {input.vars} \
            --samples {output.samples} \
            --outfile {output.vcf_gz} > {log} 2>&1
            
        if [[ "$(singularity exec --bind {params.bind} {params.sif} zcat {output.vcf_gz} | grep -v "^#" | wc -l)" -eq "0" ]]; 
        then
           echo "Error, total number of SNPs in the output VCF is 0"
           rm {output.vcf_gz}
        fi
            
        singularity exec --bind {params.bind} {params.sif} gunzip -c {output.vcf_gz} | \
            singularity exec --bind {params.bind} {params.sif} bgzip > {output.vcf_bgz}
        singularity exec --bind {params.bind} {params.sif} tabix -p vcf {output.vcf_bgz}
        """


rule filter_exp:
    input:
        data = config["inputs"]["exp"],
        gte = get_dataset_gte
    output:
        samples = temp(config["outputs"]["output_dir"] + "exp/{dataset}/samples.txt"),
        out = config["outputs"]["output_dir"] + "exp/{dataset}/" + get_basename(config["inputs"]["exp"]) + ".txt.gz"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["filter_matrix_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["filter_matrix_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["filter_matrix_time"]]
    threads: config["filter_matrix_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/filter_matrix.py",
    log: config["outputs"]["output_dir"] + "log/filter_exp.{dataset}.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} awk 'BEGIN{{ FS = OFS = "\\t" }} {{ print $2 }}' {input.gte} > {output.samples}
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --data {input.data} \
            --columns {output.samples} \
            --outfile {output.out} > {log} 2>&1
        """


rule pca:
    input:
        data = get_pca_expr,
        gte = get_dataset_gte
    output:
        pca = config["outputs"]["output_dir"] + "pca/{dataset}/{infile}.Pcs.txt.gz",
        rot = config["outputs"]["output_dir"] + "pca/{dataset}/{infile}.Pcs_rot.txt.gz",
        var = config["outputs"]["output_dir"] + "pca/{dataset}/{infile}.Pcs_var.txt.gz",
        pca_plot = config["outputs"]["output_dir"] + "pca/{dataset}/{infile}.Pcs.png",
        scree_plot = config["outputs"]["output_dir"] + "pca/{dataset}/{infile}.Scree.png",
        done = config["outputs"]["output_dir"] + "pca/{dataset}/{infile}.done",
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["pca_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["pca_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["pca_time"]]
    threads: config["pca_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/pca.py",
        out = config["outputs"]["output_dir"] + "pca/{dataset}/{infile}."
    log: config["outputs"]["output_dir"] + "log/pca.{dataset}.{infile}.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --data {input.data} \
            --gte {input.gte} \
            --transpose \
            --center \
            --scale \
            --out {params.out} > {log} 2>&1
        singularity exec --bind {params.bind} {params.sif} touch {output.done}
        """


rule filter_pcs:
    input:
        data = config["outputs"]["output_dir"] + "pca/{dataset}/" + get_basename(config["inputs"]["exp"]) + ".Pcs.txt.gz"
    output:
        out = config["outputs"]["output_dir"] + "pca/{dataset}/" + get_basename(config["inputs"]["exp"]) + ".{n_pcs}Pcs.txt.gz"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["filter_matrix_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["filter_matrix_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["filter_matrix_time"]]
    threads: config["filter_matrix_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/filter_matrix.py",
    log: config["outputs"]["output_dir"] + "log/filter_pca.{dataset}.{n_pcs}Pcs.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --data {input.data} \
            --head {wildcards.n_pcs} \
            --outfile {output.out} > {log} 2>&1
        """


rule merge_covs:
    input:
        cov = config["inputs"]["cov"],
        pca = config["outputs"]["output_dir"] + "pca/{dataset}/" + get_basename(config["inputs"]["exp"]) + ".{n_pcs}Pcs.txt.gz"
    output:
        cov = config["outputs"]["output_dir"] + "cov/{dataset}/" + get_basename(config["inputs"]["exp"]) + ".cov{n_pcs}Pcs.txt.gz"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["merge_matrices_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["merge_matrices_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["merge_matrices_time"]]
    threads: config["merge_matrices_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/merge_matrices.py",
        out = lambda wildcards: config["outputs"]["output_dir"] + "cov/" + wildcards.dataset + "/" + get_basename(config["inputs"]["exp"]) + ".cov" + wildcards.n_pcs + "Pcs"
    log: config["outputs"]["output_dir"] + "log/merge_covs.{dataset}.cov{n_pcs}Pcs.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --data {input.cov} {input.pca} \
            --axis 0 \
            --out {params.out} > {log} 2>&1
        """


rule regressor:
    input:
        data = get_regressor_exp,
        cov = get_cov
    output:
        data = config["outputs"]["output_dir"] + "regressor/{dataset}/" + get_basename(config["inputs"]["exp"]) + ".{cov}.CovariatesRemovedOLS.txt.gz"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["regressor_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["regressor_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["regressor_time"]]
    threads: config["regressor_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/regressor.py",
        out = config["outputs"]["output_dir"] + "regressor/{dataset}/" + get_basename(config["inputs"]["exp"]) + ".{cov}"
    log: config["outputs"]["output_dir"] + "log/regressor.{dataset}.{cov}.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --data {input.data} \
            --cov {input.cov} \
            --out {params.out} > {log} 2>&1
        """


rule merge_exps:
    input:
        exps = lambda wildcards: expand(config["outputs"]["output_dir"] + "regressor/{dataset}/" + get_basename(config["inputs"]["exp"]) + ".{cov}.CovariatesRemovedOLS.txt.gz", dataset=get_datasets(wildcards), allow_missing=True)
    output:
        out = config["outputs"]["output_dir"] + "regressor/merged." + get_basename(config["inputs"]["exp"]) + ".{cov}.CovariatesRemovedOLS.txt.gz"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["merge_matrices_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["merge_matrices_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["merge_matrices_time"]]
    threads: config["merge_matrices_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/merge_matrices.py",
        out = lambda wildcards: config["outputs"]["output_dir"] + "regressor/merged." + get_basename(config["inputs"]["exp"]) + "." + wildcards.cov + ".CovariatesRemovedOLS"
    log: config["outputs"]["output_dir"] + "log/merge_exps.{cov}.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --data {input.exps} \
            --axis 1 \
            --out {params.out} > {log} 2>&1
        """


rule remove_gte_dataset:
    input:
        gte = get_gte
    output:
        gte = config["outputs"]["output_dir"] + "gte/gte_noDataset.txt"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["remove_gte_dataset_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["remove_gte_dataset_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["remove_gte_dataset_time"]]
    threads: config["remove_gte_dataset_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
    log: config["outputs"]["output_dir"] + "log/remove_gte_dataset.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} awk 'BEGIN{{ FS = OFS = "\\t" }} {{ print $1, $2, "Dataset" }}' {input.gte} > {output.gte}
        """


# TODO: perhaps better to use the genelimit / snpgenelimit file here if it is given so the jobs are more
#   equally distributed.
checkpoint create_batches:
    input:
        exp = get_exp
    output:
        batches = directory(config["outputs"]["output_dir"] + "batches/{cov}/")
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["create_batches_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["create_batches_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["create_batches_time"]]
    threads: config["create_batches_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/create_batches.py",
        n_genes = config["general_settings"]["n_genes"],
        out = config["outputs"]["output_dir"] + "batches/{cov}/" + config["outputs"]["output_prefix"]
    log: config["outputs"]["output_dir"] + "log/create_batches.{cov}.log"
    shell:
        """
        mkdir -p {output.batches}
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --exp {input.exp} \
            --n_genes {params.n_genes} \
            --out {params.out} > {log} 2>&1
        """


rule run_qtl:
    input:
        annotation = get_annotation,
        snpannotation = get_snpannotation,
        vcf = get_vcf,
        index = get_vcf_index,
        exp = get_exp,
        gte = get_qtl_gte
    output:
        log = config["outputs"]["output_dir"] + "output/{cov}/{batch}-log.txt.gz",
        top = config["outputs"]["output_dir"] + "output/{cov}/{batch}-TopEffects.txt",
        all = config["outputs"]["output_dir"] + "output/{cov}/{batch}-AllEffects.txt.gz" if config["qtl_settings"]["outputall"] else temp(config["outputs"]["output_dir"] + "output/{cov}/{batch}-AllEffects.txt.gz"),
        all_perm = config["outputs"]["output_dir"] + "output/{cov}/{batch}-Permutations.txt.gz" if config["qtl_settings"]["outputallpermutations"] else temp(config["outputs"]["output_dir"] + "output/{cov}/{batch}-Permutations.txt.gz"),
        snplog = config["outputs"]["output_dir"] + "output/{cov}/{batch}-snpqclog.txt.gz" if config["qtl_settings"]["snplog"] else temp(config["outputs"]["output_dir"] + "output/{cov}/{batch}-snpqclog.txt.gz"),
        finished = config["outputs"]["output_dir"] + "output/{cov}/{batch}-TopEffects.finished"
    resources:
        java_mem_gb = lambda wildcards, attempt: attempt * config["run_qtl_memory"] * config["run_qtl_threads"] - config["settings_extra"]["java_memory_buffer"],
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["run_qtl_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["run_qtl_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["run_qtl_time"]]
    threads: config["run_qtl_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        java_threads = config["run_qtl_threads"] * 2,
        jar = config["inputs"]["mbqtl_jar"],
        analysis_type = config["qtl_settings"]["analysis_type"],
        meta_analysis_method = config["qtl_settings"]["meta_analysis_method"],
        seed = config["qtl_settings"]["seed"],
        snpannotation = lambda wildcards: "--snpannotation " + get_snpannotation(wildcards) if config["general_settings"]["use_snpannotation"] else "",
        genelimit = get_genelimit,
        snplimit = get_snplimit,
        snpgenelimit = get_snpgenelimit,
        perm = config["qtl_settings"]["perm"],
        ciswindow = config["qtl_settings"]["ciswindow"],
        maf = config["qtl_settings"]["maf"],
        cr = config["qtl_settings"]["cr"],
        hwep = config["qtl_settings"]["hwep"],
        minobservations = config["qtl_settings"]["minobservations"],
        nrdatasets = config["qtl_settings"]["nrdatasets"],
        mingenotypecount = config["qtl_settings"]["mingenotypecount"],
        splitmultiallelic = "--splitmultiallelic" if config["qtl_settings"]["splitmultiallelic"] else "",
        replacemissinggenotypes = "--replacemissinggenotypes" if config["qtl_settings"]["replacemissinggenotypes"] else "",
        usehardgenotypecalls = "--usehardgenotypecalls" if config["qtl_settings"]["usehardgenotypecalls"] else "",
        norank = "--norank" if config["qtl_settings"]["norank"] else "",
        outputall = "--outputall" if config["qtl_settings"]["outputall"] else "",
        outputallpermutations = "--outputallpermutations" if config["qtl_settings"]["outputallpermutations"]  else "",
        snplog = "--snplog" if config["qtl_settings"]["snplog"] else "",
        out = lambda wildcards: config["outputs"]["output_dir"] + "output/" + wildcards.cov + "/" + wildcards.batch
    log: config["outputs"]["output_dir"] + "log/run_qtl.{cov}.{batch}.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} java -Xmx{resources.java_mem_gb}g -Xms{resources.java_mem_gb}g \
            -Djava.util.concurrent.ForkJoinPool.common.parallelism={params.java_threads} \
            -Dmaximum.threads={params.java_threads} -Dthread.pool.size={params.java_threads} \
            -jar {params.jar} \
            --mode mbqtl \
            --{params.analysis_type} \
            --{params.meta_analysis_method} \
            --seed {params.seed} \
            --vcf {input.vcf} \
            --exp {input.exp} \
            --gte {input.gte} \
            --annotation {input.annotation} \
            {params.snpannotation} \
            {params.genelimit} \
            {params.snplimit} \
            {params.snpgenelimit} \
            --perm {params.perm} \
            --ciswindow {params.ciswindow} \
            --maf {params.maf} \
            --cr {params.cr} \
            --hwep {params.hwep} \
            --minobservations {params.minobservations} \
            --nrdatasets {params.nrdatasets} \
            --mingenotypecount {params.mingenotypecount} \
            {params.splitmultiallelic} \
            {params.replacemissinggenotypes} \
            {params.usehardgenotypecalls} \
            {params.norank} \
            {params.outputall} \
            {params.outputallpermutations} \
            {params.snplog} \
            --out {params.out} > {log} 2>&1
            
        if [[ "$(singularity exec --bind {params.bind} {params.sif} cat {output.top} | wc -l)" -le "1" ]]; 
        then
           echo "Error, no results in {output.top}"
           rm {output.top}
        fi
        
        singularity exec --bind {params.bind} {params.sif} touch {output.all}
        singularity exec --bind {params.bind} {params.sif} touch {output.all_perm}
        singularity exec --bind {params.bind} {params.sif} touch {output.snplog}
        """


rule merge:
    input:
        top = lambda wildcards: expand(config["outputs"]["output_dir"] + "output/{cov}/" + "{batch}-TopEffects.txt", batch=get_batches(wildcards), allow_missing=True)
    output:
        top = config["outputs"]["output_dir"] + "output/{cov}/" + config["outputs"]["output_prefix"] + "-merged.txt"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["merge_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["merge_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["merge_time"]]
    threads: config["merge_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/merge.py",
        input_dir = config["outputs"]["output_dir"] + "output/{cov}/" + config["outputs"]["output_prefix"],
        out = config["outputs"]["output_dir"] + "output/{cov}/" + config["outputs"]["output_prefix"] + "-merged"
    log: config["outputs"]["output_dir"] + "log/merge.{cov}.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --input_dir {params.input_dir} \
            --out {params.out} > {log} 2>&1
        """


rule qvalues:
    input:
        top_hits = get_top_hits
    output:
        top_hits = config["outputs"]["output_dir"] + "output/{cov}/" + config["outputs"]["output_prefix"] + "-TopEffectsWithqval.txt",
        fig1 = config["outputs"]["output_dir"] + "figures/{cov}/" + config["outputs"]["output_prefix"] + "_overview.pdf",
        fig2 = config["outputs"]["output_dir"] + "figures/{cov}/" + config["outputs"]["output_prefix"] + "_hist.png"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["qvalues_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["qvalues_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["qvalues_time"]]
    threads: config["qvalues_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/multiple_testing_correction.R",
        beta_dist_a = "BetaDistAlpha",
        beta_dist_b = "BetaDistBeta",
        nom_threshold = "PvalueNominalThreshold",
        pvalue = "BetaAdjustedMetaP" if config["qtl_settings"]["perm"] > 0 else "MetaP",
        qvalue = "qval",
        alpha = config["general_settings"]["alpha"],
        data_out = config["outputs"]["output_dir"] + "output/{cov}/" + config["outputs"]["output_prefix"] + "-TopEffects",
        plot_out = config["outputs"]["output_dir"] + "figures/{cov}/" + config["outputs"]["output_prefix"],
        tmp = config["outputs"]["output_dir"] + "figures/{cov}/Rplots.pdf",
        suffix = "Withqval"
    log: config["outputs"]["output_dir"] + "log/qvalues.{cov}.log"
    shell:
        """
         singularity exec --bind {params.bind} {params.sif} Rscript {params.script} \
            --input {input.top_hits} \
            --beta_dist_a {params.beta_dist_a} \
            --beta_dist_b {params.beta_dist_b} \
            --nom_threshold {params.nom_threshold} \
            --pvalue {params.pvalue} \
            --qvalue {params.qvalue} \
            --alpha {params.alpha} \
            --data_out {params.data_out} \
            --plot_out {params.plot_out} \
            --suffix {params.suffix} > {log} 2>&1
        singularity exec --bind {params.bind} {params.sif} mv {params.tmp} {output.fig1}
        """


rule results:
    input:
        data = expand(config["outputs"]["output_dir"] + "output/{cov}/" + config["outputs"]["output_prefix"] + "-TopEffectsWithqval.txt", cov=covariates)
    output:
        out = config["outputs"]["output_dir"] + "output/" + config["outputs"]["output_prefix"] + "-results.txt"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["results_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["results_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["results_time"]]
    threads: config["results_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/results.py",
        data = config["outputs"]["output_dir"] + "output/*/" + config["outputs"]["output_prefix"] + "-TopEffectsWithqval.txt",
        nom_pvalue_column = "MetaP",
        perm_pvalue_column = "BetaAdjustedMetaP",
        qvalue_column = "qval",
        minimimal_reporting_p = config["general_settings"]["alpha"]
    log: config["outputs"]["output_dir"] + "log/results.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --data '{params.data}' \
            --nom_pvalue_column {params.nom_pvalue_column} \
            --perm_pvalue_column {params.perm_pvalue_column} \
            --qvalue_column {params.qvalue_column} \
            --minimimal_reporting_p {params.minimimal_reporting_p} \
            --out {output.out} > {log} 2>&1
        """

