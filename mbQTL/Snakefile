#!/usr/bin/env python
import os

# Set logger level.
if config["settings_extra"]["debug"]:
    logger.set_level("DEBUG")

# Validate input.
if config["general_settings"]["n_genes"] is not None and config["general_settings"]["n_genes"] <= 0:
    logger.error("Critical, n_genes needs to be larger than zero.\n\nExiting.")
    exit("InvalidInput")

# Check required input arguments.
if config["inputs"]["singularity_image"] is None or not os.path.exists(config["inputs"]["singularity_image"]):
    logger.error("Critical, singularity_image does not exist.\n\nExiting.")
    exit("MissingsingularityImage")
if config["inputs"]["repo_dir"] is None or not os.path.exists(config["inputs"]["repo_dir"]):
    logger.error("Critical, repo_dir does not exist.\n\nExiting.")
    exit("MissingRepoDir")
if config["inputs"]["mbqtl_jar"] is None or not os.path.exists(config["inputs"]["mbqtl_jar"]):
    logger.error("Critical, mbqtl_jar does not exist.\n\nExiting.")
    exit("MissingsingmbQTLJar")
if config["inputs"]["annotation"] is None or not os.path.exists(config["inputs"]["annotation"]):
    logger.error("Critical, annotation does not exist.\n\nExiting.")
    exit("MissingsingAnnotation")
if config["inputs"]["vcf"] is None or not os.path.exists(config["inputs"]["vcf"]):
    logger.error("Critical, vcf does not exist.\n\nExiting.")
    exit("MissingsingVCF")
if config["inputs"]["exp"] is None or not os.path.exists(config["inputs"]["exp"]):
    logger.error("Critical, exp does not exist.\n\nExiting.")
    exit("MissingsingExp")
if config["inputs"]["gte"] is None or not os.path.exists(config["inputs"]["gte"]):
    logger.error("Critical, gte does not exist.\n\nExiting.")
    exit("MissingsingGTE")
if config["outputs"]["output_dir"] is None:
    logger.error("Critical, the output_dir cannot be empty.\n\nExiting.")
    exit("MissingOutputDir")

# Check optional input arguments.
if config["inputs"]["cov"] is not None and not os.path.exists(config["inputs"]["cov"]):
    logger.error("Critical, cov does not exist.\n\nExiting.")
    exit("InvalidCov")
if config["inputs"]["genelimit"] is not None and not os.path.exists(config["inputs"]["genelimit"]):
    logger.error("Critical, genelimit does not exist.\n\nExiting.")
    exit("InvalidGeneLimit")
if config["inputs"]["snplimit"] is not None and not os.path.exists(config["inputs"]["snplimit"]):
    logger.error("Critical, snplimit does not exist.\n\nExiting.")
    exit("InvalidSNPLimit")
if config["inputs"]["snpgenelimit"] is not None and not os.path.exists(config["inputs"]["snpgenelimit"]):
    logger.error("Critical, snpgenelimit does not exist.\n\nExiting.")
    exit("InvalidSNPGeneLimit")

# Checking settings with options to be valid.
if config["general_settings"]["force_mega"] not in ["all", "cov", "qtl", "none", None]:
    logger.error("Critical, force_mega must be 'all', 'cov', 'qtl', 'none', or empty.\n\nExiting.")
    exit("InvalidForceMegaSetting")
if config["qtl_settings"]["analysis_type"] not in ["cis", "trans", "cistrans"]:
    logger.error("Critical, analysis_type must be 'cis', 'trans', or 'cistrans'.\n\nExiting.")
    exit("InvalidAnalysisType")
if config["qtl_settings"]["meta_analysis_method"] not in ["empzmeta", "fisherzmeta", "fisherzmetarandom"]:
    logger.error("Critical, meta_analysis_method must be 'empzmeta', 'fisherzmeta', or 'fisherzmetarandom'.\n\nExiting.")
    exit("InvalidMetaAnalysisMethod")

# Replace non required variables that are None with empty strings.
for input_variable in ["cov", "genelimit", "snplimit", "snpgenelimit"]:
    if config["inputs"][input_variable] is None:
        config["inputs"][input_variable] = ""

# Set the default output prefix.
if config["outputs"]["output_prefix"] is None:
    config["outputs"]["output_prefix"] = "mbQTL"

# Reformat the selected number of PCs into a list.
if config["general_settings"]["n_pcs"] is None:
    config["general_settings"]["n_pcs"] = []
if not isinstance(config["general_settings"]["n_pcs"], list):
    config["general_settings"]["n_pcs"] = [config["general_settings"]["n_pcs"]]

# Reformat force mega in a list and replace 'all' with all options.
if config["general_settings"]["force_mega"] == "all":
    config["general_settings"]["force_mega"] = ["cov", "qtl"]
else:
    config["general_settings"]["force_mega"] = [config["general_settings"]["force_mega"]]

# Add trailing /.
if not config["inputs"]["repo_dir"].endswith("/"):
    config["inputs"]["repo_dir"] += "/"
if not config["outputs"]["output_dir"].endswith("/"):
    config["outputs"]["output_dir"] += "/"

# Check setting combinations that do not make sense and update accordingly.
if config["settings_extra"]["force"]:
    logger.warning("Warning, unlogical input arguments will not be automatically updated. Use with caution!")
else:
    if (config["inputs"]["snpgenelimit"] != "" or config["inputs"]["snplimit"] != "") and config["general_settings"]["use_snpannotation"]:
        logger.warning("Warning, setting use_snpannotation to False since snpgenelimit / snplimit is used.")
        config["general_settings"]["use_snpannotation"] = False
    if config["inputs"]["snpgenelimit"] == "" and config["inputs"]["snplimit"] == "" and config["general_settings"]["filter_vcf"]:
        logger.warning("Warning, setting filter_vcf to False since snpgenelimit / snplimit is not used.")
        config["general_settings"]["filter_vcf"] = False
    if config["inputs"]["snpgenelimit"] != "" and not config["qtl_settings"]["outputall"]:
        logger.warning("Warning, setting outputall to True since snpgenelimit is used.")
        config["qtl_settings"]["outputall"] = True
    if config["inputs"]["snpgenelimit"] != "" and not config["qtl_settings"]["perm"] == 0:
        logger.warning("Warning, setting perm to 0 since snpgenelimit is used.")
        config["qtl_settings"]["perm"] = 0
    if config["inputs"]["gte"].endswith(".smf") and config["qtl_settings"]["nrdatasets"] > 1:
        logger.warning("Warning, setting nrdatasets to 1 since the input gte is a single dataset ('.smf').")
        config["qtl_settings"]["nrdatasets"] = 1
    if "qtl" in config["general_settings"]["force_mega"] and config["qtl_settings"]["nrdatasets"] > 1:
        logger.warning("Warning, setting nrdatasets to 1 since force_mega for QTL mapping is True.")
        config["qtl_settings"]["nrdatasets"] = 1

# Check setting combinations that are not allowed.
if config["qtl_settings"]["nrdatasets"] > 1 and config["inputs"]["gte"].endswith(".smf"):
    logger.error("Critical, setting the minimum number of datasets required in "
                 "meta-analysis (nrdatasets) > 1 will result in no results "
                 "since your input GTE has no dataset column.")
    exit()
if config["qtl_settings"]["replacemissinggenotypes"] and config["qtl_settings"]["perm"] == 0:
    logger.error("Critical, replacemissinggenotypes is True while perm is 0. This setting should "
                 "only be True when both genotypes and expression data have missing "
                 "values and perm > 0.\n\nExiting.")
    exit()

# Some friendly reminders.
if config["qtl_settings"]["nrdatasets"] > 1:
    logger.error("Error, setting the minimum number of datasets required in "
                  "meta-analysis (nrdatasets) > 1 will result in no results "
                  "if your input GTE only contains one dataset.")
if config["inputs"]["gte"].endswith(".smf"):
    logger.warning("Warning, your input gte ends with '.smf'. If there is any datasets info in "
                   "the third column it will be ignored and all analyses will be performed over "
                   "all samples at once. This is equivalent to setting force_mega: all.")
else:
    logger.warning("Warning, your input gte ends with '.txt'. The info in the third column will "
                   "be used as dataset. The principal component calculations and corrections "
                   "will be performed per dataset. If there is only one dataset, all analyses will "
                   "be performed over all samples at once. Use force_mega: cov to force the "
                   "program to perform PCA correction over all samples.")
if config["inputs"]["snpgenelimit"] != "" and config["run_qtl_time"] != 0:
    logger.warning("Warning, you are only testing a select number of effects but the runtime for each "
                   "job is max {}. If you tink this is too long you should decrease "
                   "run_qtl_time to e.g. 0.".format(config["cluster_time"][config["run_qtl_time"]]))
if config["inputs"]["snpgenelimit"] == "" and config["general_settings"]["n_genes"] is None and config["run_qtl_time"] == 0:
    logger.warning("Warning, you are running all tests in one job file but the runtime for this "
                   "job is max {}. If you tink this is too short you should increase "
                   "run_qtl_time to 1 or higher.".format(config["cluster_time"][config["run_qtl_time"]]))
if config["inputs"]["snpgenelimit"] == "" and config["general_settings"]["n_genes"] is not None and config["run_qtl_time"] != 0:
    logger.warning("Warning, you are running tests in multiple job files but the runtime for each "
                   "job is max {}. If you tink this is too long you should decrease "
                   "run_qtl_time to e.g. 0.".format(config["cluster_time"][config["run_qtl_time"]]))

# Automatically determine the mode of the input.
input_mode = "default"
if config["inputs"]["cov"] != "" and not config["general_settings"]["n_pcs"]:
    input_mode = "cov"
elif config["inputs"]["cov"] != "" and config["general_settings"]["n_pcs"]:
    input_mode = "covXPcs"
elif config["inputs"]["cov"] == "" and config["general_settings"]["n_pcs"]:
    input_mode = "XPcs"
else:
    logger.error("Critical, could not determine input mode.\n\nExiting")
    exit()
logger.info("Your input is in mode: {}.".format(input_mode))

# Define the tests that the user wants to run.
modes = ["all", "default", "cov", "covXPcs", "XPcs"]
selected_modes = set()
if config["general_settings"]["include_modes"] is None:
    # No user input defaults to the input mode.
    config["general_settings"]["include_modes"] = input_mode
if not isinstance(config["general_settings"]["include_modes"], list):
    # Force input into a list.
    config["general_settings"]["include_modes"] = [config["general_settings"]["include_modes"]]
for mode in config["general_settings"]["include_modes"]:
    # Validate that each mode is known.
    if mode not in modes:
        logger.error("Critical, unexpected mode '{}'.\n\nExiting.".format(mode))
        exit()
# Construct the selected modes.
if "all" in config["general_settings"]["include_modes"]:
    selected_modes = set(modes)
else:
    selected_modes = set(config["general_settings"]["include_modes"])
logger.info("You selected input to execute the following modes: {}.".format(", ".join(selected_modes)))

# Combine the output modes (henceforth named covariates).
covariates = []
if "default" in selected_modes:
    covariates.append("default")
if "cov" in selected_modes:
    if config["inputs"]["cov"] == "":
        logger.warning("Warning, you requested to run mode 'cov' but the 'cov' input is empty. Skipping mode.")
    else:
        covariates.append("cov")
if "XPcs" in selected_modes:
    if not config["general_settings"]["n_pcs"]:
        logger.warning("Warning, you requested to run mode 'XPcs' but the 'n_pcs' input is empty. Skipping mode.")
    else:
        for n_pcs in config["general_settings"]["n_pcs"]:
            if n_pcs == 0 and "default" not in covariates:
                covariates.append("default")
                continue
            covariates.append(str(n_pcs) + "Pcs")
if "covXPcs" in selected_modes:
    if config["inputs"]["cov"] == "":
        logger.warning("Warning, you requested to run mode 'covXPcs' but the 'cov' input is empty. Skipping mode.")
    elif not config["general_settings"]["n_pcs"]:
        logger.warning("Warning, you requested to run mode 'covXPcs' but the 'n_pcs' input is empty. Skipping mode.")
    else:
        for n_pcs in config["general_settings"]["n_pcs"]:
            if n_pcs == 0 and "cov" not in covariates:
                covariates.append("cov")
                continue
            covariates.append("cov" + str(n_pcs) + "Pcs")
if len(covariates) > 0:
    logger.info("Generating output for the following modes: {}.".format(", ".join(covariates)))


wildcard_constraints:
    n_pcs = "[0-9]+"


def get_basename(fpath):
    return os.path.basename(fpath).rstrip(".bgz").rstrip(".gz").rstrip(".txt").rstrip(".tsv").rstrip(".csv").rstrip(".vcf")


def print_wildcards(wildcards):
    out = []
    for key, value in wildcards.items():
        out.append(key + "=" + value)
    return ", ".join(out)


def get_input(wildcards):
    """
    :rule: all
    :param wildcards:
    :return: the required input files.
    """
    logger.debug("rule all - get_input({})".format(print_wildcards(wildcards)))
    input = []
    exp_basename = get_basename(config["inputs"]["exp"])

    if config["general_settings"]["plot_pca"]:
        input_fpath = "pca/all/{exp_basename}.done".format(**wildcards, exp_basename=exp_basename)
        logger.debug("\trequesting output of rule 'pca' on the input 'exp': " + input_fpath)
        input.append(config["outputs"]["output_dir"] + input_fpath)

        if "cov" in config["general_settings"]["force_mega"]:
            input_fpaths = expand(config["outputs"]["output_dir"] + "pca/all/{exp_basename}.{cov}.CovariatesRemovedOLS.done", exp_basename=exp_basename, cov=covariates)
            logger.debug("\trequesting output of rule 'pca' on the output of rule 'regressor': " + ", ".join([fpath.replace(config["outputs"]["output_dir"], "") for fpath in input_fpaths]))
            input.extend(input_fpaths)
        else:
            if "cov" in covariates:
                input_fpath = "pca/all/{exp_basename}.cov.CovariatesRemovedOLS.done".format(**wildcards, exp_basename=exp_basename)
                logger.debug("\trequesting output of rule 'pca' on the output of rule 'regressor': " + input_fpath)
                input.append(config["outputs"]["output_dir"] + input_fpath)

            # Per dataset PCA output is not required if (1) the number of PCs removed is >0, and
            # (2) the input file is not smf since those filetypes are guaranteed to have 1 dataset.
            ds_corr_covariates = [cov for cov in covariates if cov not in ["default", "cov", "0Pcs", "cov0Pcs"]]
            if len(ds_corr_covariates) > 0 and not config["inputs"]["gte"].endswith(".smf"):
                datasets = get_datasets(wildcards)

                # No need to merge if there is only one dataset.
                if len(get_datasets(wildcards)) == 1:
                    input_fpaths = expand(config["outputs"]["output_dir"] + "pca/all/{exp_basename}.{cov}.CovariatesRemovedOLS.done", exp_basename=exp_basename, cov=ds_corr_covariates)
                    logger.debug("\trequesting output of rule 'pca' on the output of rule 'regressor' since there is 1 dataset: " + ", ".join([fpath.replace(config["outputs"]["output_dir"], "") for fpath in input_fpaths]))
                    input.extend(input_fpaths)
                else:
                    input_fpaths = expand(config["outputs"]["output_dir"] + "pca/{dataset}/{exp_basename}.done", dataset=datasets, exp_basename=exp_basename)
                    logger.debug("\trequesting output of rule 'pca' on the output of rule 'filter_exp' for a specific dataset: " + ", ".join([fpath.replace(config["outputs"]["output_dir"], "") for fpath in input_fpaths]))
                    input.extend(input_fpaths)

                    input_fpaths = expand(config["outputs"]["output_dir"] + "pca/{dataset}/{exp_basename}.{cov}.CovariatesRemovedOLS.done", dataset=datasets, exp_basename=exp_basename, cov=ds_corr_covariates)
                    logger.debug("\trequesting output of rule 'pca' on the output of rule 'regressor' for a specific dataset: " + ", ".join([fpath.replace(config["outputs"]["output_dir"], "") for fpath in input_fpaths]))
                    input.extend(input_fpaths)

                    input_fpaths = expand(config["outputs"]["output_dir"] + "pca/merged/{exp_basename}.{cov}.CovariatesRemovedOLS.done", exp_basename=exp_basename, cov=ds_corr_covariates)
                    logger.debug("\trequesting output of rule 'pca' on the output of rule 'merge_exps': " + ", ".join([fpath.replace(config["outputs"]["output_dir"], "") for fpath in input_fpaths]))
                    input.extend(input_fpaths)

    # Output of mbQTL.
    input_fpaths = expand(config["outputs"]["output_dir"] + "output/{cov}/{output_prefix}-TopEffectsWithqval.txt", cov=covariates, output_prefix=config["outputs"]["output_prefix"])
    logger.debug("\trequesting output of rule 'run_qtl': " + ", ".join([fpath.replace(config["outputs"]["output_dir"], "") for fpath in input_fpaths]))
    input.extend(input_fpaths)

    # Output of results.
    if covariates:
        input_fpath = "output/{output_prefix}-results.txt".format(**wildcards, output_prefix=config["outputs"]["output_prefix"])
        logger.debug("\trequesting output of rule 'results': " + input_fpath)
        input.append(config["outputs"]["output_dir"] + input_fpath)

    return input


def get_gte(wildcards):
    """
    :rule: split_gte, filter_exp, pca, remove_gte_dataset, and run_qtl
    :param wildcards:
    :return: return the output of smf_to_gte if the inpu gte is a '.smf' file, else return the input gte.
    """
    logger.debug("rule split_gte, filter_exp, pca, remove_gte_dataset, run_qtl - get_gte({})".format(print_wildcards(wildcards)))
    if config["inputs"]["gte"].endswith(".smf"):
        fpath = "gte/gte.txt"
        logger.debug("\treturn output of rule 'smf_to_gte': " + fpath)
        return config["outputs"]["output_dir"] + fpath

    fpath = config["inputs"]["gte"]
    logger.debug("\treturn input 'gte': " + fpath)
    return fpath


def get_variants(wildcards):
    """
    :rule: filter_vcf
    :param wildcards:
    :return: return the snpgenelimit if it is given, else the snplimit if it is given.
    """
    logger.debug("rule filter_vcf - get_variants({})".format(print_wildcards(wildcards)))

    if wildcards.limit_file == get_basename(config["inputs"]["snpgenelimit"]):
        fpath = "limit/{sgl_basename}_variants.txt".format(**wildcards, sgl_basename=get_basename(config["inputs"]["snpgenelimit"]))
        logger.debug("\treturn output of rule 'split_snpgenelimit': " + fpath)
        return config["outputs"]["output_dir"] + fpath
    elif wildcards.limit_file == get_basename(config["inputs"]["snplimit"]):
        fpath = config["inputs"]["snplimit"]
        logger.debug("\treturn input 'snplimit': " + fpath)
        return fpath
    else:
        logger.error("Critical, Unexpected value for limit_file in get_variants: {}\n\nExiting.".format(wildcards.limit_file))
        exit()

    logger.debug("\treturning []")
    return []

def get_dataset_gte(wildcards):
    """
    :rule: pca and filter_exp
    :param wildcards:
    :return: return the gte filtered on a specific dataset unless the dataset is merged / all in which case the whole gte
    is returned.
    """
    logger.debug("rule pca, filter_exp - get_dataset_gte({})".format(print_wildcards(wildcards)))
    if wildcards.dataset in ["merged", "all"]:
        logger.debug("\tcalling 'get_gte' function")
        fpath = get_gte(wildcards)
        logger.debug("\treturning: " + fpath)
        return fpath

    fpath = "gte/split/gte_{dataset}.txt".format(**wildcards)
    logger.debug("\treturn output of checkpoint 'split_gte': " + fpath)
    return config["outputs"]["output_dir"] + fpath


def get_pca_expr(wildcards):
    """
    :rule: pca
    :param wildcards:
    :return: determine the input expression file over which to perform PCA. If the infile is the input exp basename
    we know that no covariates should be corrected so we can return either the whole matrix (exp input) or a filtered
    version based on the dataset (filter_exp). Else, we need a covariate corrected matrix. If the dataset is merged this
    means we need to combine the regressor output of multiple datasets from merge_exps. Else, we turn the regressor output
    directly for that dataset.
    """
    logger.debug("rule pca - get_pca_expr({})".format(print_wildcards(wildcards)))

    if wildcards.infile == get_basename(config["inputs"]["exp"]):
        # i.e. no covariate correction is needed. Only need to pick between the
        # full input matrix or a subset of the input matrix.
        if wildcards.dataset == "all" or "cov" in config["general_settings"]["force_mega"] or config["inputs"]["gte"].endswith(".smf") or len(get_datasets(wildcards)) == 1:
            fpath = config["inputs"]["exp"]
            logger.debug("\treturning input 'exp': " + fpath)
            return fpath

        fpath = "exp/{dataset}/{infile}.txt.gz".format(**wildcards)
        logger.debug("\treturning output of rule 'filter_exp' for a specific dataset: " + fpath)
        return config["outputs"]["output_dir"] + fpath

    # covariate corrected per dataset merged into one file.
    if wildcards.dataset == "merged":
        fpath = "regressor/merged.{infile}.txt.gz".format(**wildcards)
        logger.debug("\treturning output of rule 'merge_exps': " + fpath)
        return config["outputs"]["output_dir"] + fpath

    # else, covariate corrected per dataset.
    fpath = "regressor/{dataset}/{infile}.txt.gz".format(**wildcards)
    logger.debug("\treturning output of rule 'regressor': " + fpath)
    return config["outputs"]["output_dir"] + fpath


def get_regressor_exp(wildcards):
    """
    :rule: regressor
    :param wildcards:
    :return: return the input expression. If dataset is all we return the input exp file. Else, we return the
    output of filter_exp for that dataset.
    """
    logger.debug("rule regressor - get_regressor_exp({})".format(print_wildcards(wildcards)))
    if wildcards.dataset == "all":
        fpath = config["inputs"]["exp"]
        logger.debug("\treturning input 'exp': " + fpath)
        return fpath

    fpath = "exp/{dataset}/{exp_basename}.txt.gz".format(**wildcards, exp_basename=get_basename(config["inputs"]["exp"]))
    logger.debug("\treturning output of rule 'filter_exp' for a specific dataset: " + fpath)
    return config["outputs"]["output_dir"] + fpath


def get_cov(wildcards):
    """
    :rule: regressor
    :param wildcards:
    :return: depending on the covariates the input is selected. No covariates should not happen so an error is thrown. If
    cov contains only covariates and no PCs (i.e. cov or cov0Pcs) the input covariate file is returned. If cov contains only
    Pcs and no covariates the output of pca is returned. If cov contains both covariates and PCs the output of merge_covs is returned.
    """
    logger.debug("rule regressor - get_cov({})".format(print_wildcards(wildcards)))
    if wildcards.cov == "default" or wildcards.cov == "0Pcs":
        logger.error("Critical, unexpected covariate '{}' wildcard in get_cov.\n\nExiting.".format(wildcards.cov))
        exit()
    if wildcards.cov == "cov" or wildcards.cov == "cov0Pcs":
        # Only cov, no PCs.
        fpath = config["inputs"]["cov"]
        logger.debug("\treturning input 'cov': " + fpath)
        return fpath
    elif not wildcards.cov.startswith("cov") and wildcards.cov.endswith("Pcs"):
        # No Cov, only PCs.
        fpath = "pca/{dataset}/{exp_basename}.{cov}.txt.gz".format(**wildcards, exp_basename=get_basename(config["inputs"]["exp"]))
        logger.debug("\treturning output of rule 'pca' for a specific dataset: " + fpath)
        return config["outputs"]["output_dir"] + fpath

    # Cov and PCs.
    fpath = "cov/{dataset}/{exp_basename}.{cov}.txt.gz".format(**wildcards, exp_basename=get_basename(config["inputs"]["exp"]))
    logger.debug("\treturning output of rule 'merge_covs' for a specific dataset: " + fpath)
    return config["outputs"]["output_dir"] + fpath


def get_datasets(wildcards):
    """
    :rule: run_qtl and merge_exps
    :param wildcards:
    :return: Extract the datasets we created in the checkpoint split_gte.
    """
    logger.debug("rule run_qtl, merge_exps - get_datasets({})".format(print_wildcards(wildcards)))
    out_dir = checkpoints.split_gte.get(**wildcards).output[0]
    datasets = glob_wildcards(os.path.join(out_dir, "gte_{dataset}.txt"))
    logger.debug("\tfound datasets: '" + ", ".join(datasets.dataset) + "'")

    return datasets.dataset


def get_genes(wildcards):
    """
    :rule: create_batches and merge
    :param wildcards:
    :return: the set of genes over which to create the batches. If snpgenelimit is given, return the gene column, else if
    genelimit is given return that, else return the QTL expression matrix.
    """
    logger.debug("rule create_batches - get_genes({})".format(print_wildcards(wildcards)))

    # Creating gene batches based on snpgenelimit has prio over genelimit.
    if config["inputs"]["snpgenelimit"] != "":
        fpath = "limit/{sgl_basename}_genes.txt".format(**wildcards, sgl_basename=get_basename(config["inputs"]["snpgenelimit"]))
        logger.debug("\treturn output of rule 'split_snpgenelimit': " + fpath)
        return config["outputs"]["output_dir"] + fpath
    elif config["inputs"]["genelimit"] != "":
        fpath = config["inputs"]["genelimit"]
        logger.debug("\treturn input 'genelimit': " + fpath)
        return fpath

    logger.debug("\tcalling 'get_exp' function")
    fpath = get_exp(wildcards)
    logger.debug("\treturning: " + fpath)
    return fpath

def get_genes_file(wildcards):
    """
    :rule: run_qtl and merge
    :param wildcards:
    :return: return the basename of the file that was used to create the chunks
    """
    logger.debug("rule run_qtl, merge - get_genes_file({})".format(print_wildcards(wildcards)))
    logger.debug("\tcalling 'get_genes' function")
    fpath = get_basename(get_genes(wildcards))
    if config["general_settings"]["n_genes"] is not None:
        fpath += (str(config["general_settings"]["n_genes"]) + "Genes")
    logger.debug("\treturning: " + fpath)
    return fpath

def get_annotation(wildcards):
    """
    :rule: run_qtl
    :param wildcards:
    :return: return the gene annotation file. If the input is a gtf we need to convert this first using
    create_gene_annotation, else we return the input annotation file.
    """
    logger.debug("rule run_qtl - get_annotation({})".format(print_wildcards(wildcards)))
    if config["inputs"]["annotation"].endswith(".gtf"):
        fpath = "create_annotation/refdata-gex-GeneAnnotation.txt.gz"
        logger.debug("\treturning output of rule 'create_gene_annotation': " + fpath)
        return config["outputs"]["output_dir"] + fpath

    fpath = config["inputs"]["annotation"]
    logger.debug("\treturning input 'annotation': " + fpath)
    return fpath


def get_vcf(wildcards):
    """
    :rule: run_qtl
    :param wildcards:
    :return: return the filtered vcf if filter_vcf is True. To differentiate between snpgenelimit and snplimit based filtered VCF
    the basename of the file is added to the filtered VCF file. Else return the original VCF.
    """
    logger.debug("rule run_qtl - get_vcf({})".format(print_wildcards(wildcards)))
    if config["general_settings"]["filter_vcf"] and config["inputs"]["snpgenelimit"] != "":
        fpath = "genotype/{vcf_basename}_{sgl_basename}.vcf.bgz".format(**wildcards, vcf_basename=get_basename(config["inputs"]["vcf"]), sgl_basename=get_basename(config["inputs"]["snpgenelimit"]))
        logger.debug("\treturning output of rule 'filter_vcf' with snpgenelimit: " + fpath)
        return config["outputs"]["output_dir"] + fpath
    elif config["general_settings"]["filter_vcf"] and config["inputs"]["snplimit"] != "":
        fpath = "genotype/{vcf_basename}_{snp_basename}.vcf.bgz".format(**wildcards, vcf_basename=get_basename(config["inputs"]["vcf"]), snp_basename=get_basename(config["inputs"]["snplimit"]))
        logger.debug("\treturning output of rule 'filter_vcf' with snplimit: " + fpath)
        return config["outputs"]["output_dir"] + fpath

    fpath = config["inputs"]["vcf"]
    logger.debug("\treturning input 'vcf': " + fpath)
    return fpath


def get_vcf_index(wildcards):
    """
    :rule: run_qtl
    :param wildcards:
    :return: return the input vcf and add a 'tbi' suffix.
    """
    logger.debug("rule run_qtl - get_vcf_index({})".format(print_wildcards(wildcards)))
    logger.debug("\tcalling 'get_vcf' function")
    fpath = get_vcf(wildcards) + ".tbi"
    logger.debug("\treturning: " + fpath)
    return fpath


def get_exp(wildcards):
    """
    :rule: run_qtl
    :param wildcards:
    :return: depending on the covariates the input is selected. If no covariates are removed (i.e. default or 0Pcs) the originak
    input file can be returned. If covariates should be removed, we need to determine if this is done per dataset
    (output rule merge_exps) or not (output rule regressor).
    """
    logger.debug("rule run_qtl - get_exp({})".format(print_wildcards(wildcards)))
    if wildcards.cov == "default" or wildcards.cov == "0Pcs":
        fpath = config["inputs"]["exp"]
        logger.debug("\treturning input 'exp': " + fpath)
        return fpath
    elif wildcards.cov == "cov" or "cov" in config["general_settings"]["force_mega"] or config["inputs"]["gte"].endswith(".smf") or len(get_datasets(wildcards)) == 1:
        # wildcards == cov means there are no PCs so no per dataset correction needed
        # force_mega == cov means we want to force to regress everything together so no per dataset correction needed
        # gte endswith '.smf' means there are no datasets so no per dataset correction needed
        # n datasets == 1 means there are no other datasets so no per dataset correction needed
        fpath = "regressor/all/{exp_basename}.{cov}.CovariatesRemovedOLS.txt.gz".format(**wildcards, exp_basename=get_basename(config["inputs"]["exp"]))
        logger.debug("\treturning output of rule 'regressor': " + fpath)
        return config["outputs"]["output_dir"] + fpath

    fpath = "regressor/merged.{exp_basename}.{cov}.CovariatesRemovedOLS.txt.gz".format(**wildcards, exp_basename=get_basename(config["inputs"]["exp"]))
    logger.debug("\treturning output of rule 'merge_exps': " + fpath)
    return config["outputs"]["output_dir"] + fpath


def get_qtl_gte(wildcards):
    """
    :rule: run_qtl
    :param wildcards:
    :return: if force_mega should be applied over the QTL analysis we return a gte file where the dataset
    column is equal to 'Dataset' for all samples, else we return the original gte.
    """
    logger.debug("rule run_qtl - get_qtl_gte({})".format(print_wildcards(wildcards)))
    if "qtl" in config["general_settings"]["force_mega"]:
        fpath = "gte/gte_noDataset.txt"
        logger.debug("\treturn output of rule 'remove_gte_dataset': " + fpath)
        return config["outputs"]["output_dir"] + fpath

    logger.debug("\tcalling 'get_gte' function")
    fpath = get_gte(wildcards)
    logger.debug("\treturning: " + fpath)
    return fpath


def get_snpannotation(wildcards):
    """
    :rule: run_qtl
    :param wildcards:
    :return: '--snpannotation <snpannotation>' if use_snpannotation is True, else return an empty string.
    """
    logger.debug("rule run_qtl - get_snpannotation({})".format(print_wildcards(wildcards)))
    prefix = "--snpannotation "

    if config["general_settings"]["use_snpannotation"]:
        logger.debug("\tcalling 'get_vcf' function")
        fpath = "create_annotation/{vcf_basename}_snpannotation.txt.gz".format(**wildcards, vcf_basename=get_basename(get_vcf(wildcards)))
        logger.debug("\treturning output of rule 'create_snp_annotation': " + fpath)
        return prefix + config["outputs"]["output_dir"] + fpath

    logger.debug("\treturning ''")
    return ""


def get_genelimit(wildcards):
    """
    :rule: run_qtl
    :param wildcards:
    :return: '--genelimit <batch/genelimit>', if n_genes is higher than 1 fill in the set of genes of the
    current batch from 'create_batches', else fill in the '--genelimit <genelimit>' if it is given, else
    return an empty string.
    """
    logger.debug("rule run_qtl - get_genelimit({})".format(print_wildcards(wildcards)))
    prefix = "--genelimit "

    if config["general_settings"]["n_genes"] is not None and config["general_settings"]["n_genes"] > 1:
        # The create_batches rule takes into account the genes from genelimit
        # if they are given.
        logger.debug("\tcalling 'get_genes_file' function")
        genes_file = get_genes_file(wildcards)

        fpath = "batches/{genes_file}/{cov}/{batch}-genes.txt".format(**wildcards, genes_file=genes_file)
        logger.debug("\treturning output of checkpoint 'create_batches': " + fpath)
        return prefix + config["outputs"]["output_dir"] + fpath
    elif config["inputs"]["genelimit"] != "":
        # Only if ngenes is None: i.e. all genes are tested in 1 job
        fpath = config["inputs"]["genelimit"]
        logger.debug("\treturning input 'genelimit': " + fpath)
        return prefix + fpath

    logger.debug("\treturning ''")
    return ""

def get_snplimit(wildcards):
    """
    :rule: run_qtl
    :param wildcards:
    :return: '--snplimit <snplimit>' if it is given, else return an empty string.
    """
    logger.debug("rule run_qtl - get_snplimit({})".format(print_wildcards(wildcards)))
    prefix = "--snplimit "
    if config["inputs"]["snplimit"] != "":
        fpath = config["inputs"]["snplimit"]
        logger.debug("\treturning input 'snplimit': " + fpath)
        return prefix + fpath

    logger.debug("\treturning ''")
    return ""

def get_snpgenelimit(wildcards):
    """
    :rule: run_qtl
    :param wildcards:
    :return: '--snpgenelimit <snpgenelimit>' if it is given, else return an empty string.
    """
    logger.debug("rule run_qtl - get_snpgenelimit({})".format(print_wildcards(wildcards)))
    prefix = "--snpgenelimit "
    if config["inputs"]["snpgenelimit"] != "":
        fpath = config["inputs"]["snpgenelimit"]
        logger.debug("\treturning input 'snpgenelimit': " + fpath)
        return prefix + fpath

    logger.debug("\treturning ''")
    return ""

def get_batches(wildcards):
    """
    :rule: merge
    :param wildcards:
    :return: Extract the batches we created in the checkpoint create_batches.
    """
    logger.debug("rule merge - get_batches({})".format(print_wildcards(wildcards)))
    logger.debug("\tcalling 'get_genes_file' function")
    genes_file = get_genes_file(wildcards)

    out_dir = checkpoints.create_batches.get(**wildcards, genes_file=genes_file).output[0]
    batches = glob_wildcards(os.path.join(out_dir, "{batch}-genes.txt"))
    logger.debug("\tfound batches: '" + ", ".join(batches.batch) + "'")

    if len(batches.batch) == 0:
        logger.error("Critical, no batches found.")
        exit()

    return batches.batch


def get_top_hits(wildcards):
    """
    :rule: qvalues
    :param wildcards:
    :return: If the program is executed with a single batch we can return the output of rule run_qtl directly,
    otherwise we first need to combine all top effects by requesting the output of rule merge.
    """
    logger.debug("rule run_qtl - get_top_hits({})".format(print_wildcards(wildcards)))
    if config["general_settings"]["n_genes"] is None or config["general_settings"]["n_genes"] == 1:
        fpath = "output/{cov}/{output_prefix}-TopEffects.txt".format(**wildcards, output_prefix=config["outputs"]["output_prefix"])
        logger.debug("\treturning output of rule 'run_qtl': " + fpath)
        return config["outputs"]["output_dir"] + fpath

    fpath = "output/{cov}/{output_prefix}-merged.txt".format(**wildcards, output_prefix=config["outputs"]["output_prefix"])
    logger.debug("\treturning output of rule 'merge': " + fpath)
    return config["outputs"]["output_dir"] + fpath


rule all:
    input: get_input


rule create_gene_annotation:
    input:
        gtf = config["inputs"]["annotation"]
    output:
        dupl_annotation = temp(config["outputs"]["output_dir"] + "create_annotation/refdata-gex-GeneAnnotation.WithDuplicates.txt.gz"),
        annotation = config["outputs"]["output_dir"] + "create_annotation/refdata-gex-GeneAnnotation.txt.gz"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["create_gene_annotation_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["create_gene_annotation_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["create_gene_annotation_time"]]
    threads: config["create_gene_annotation_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/create_annotation_file.py",
        feature_name = config["create_annotation_settings"]["feature_name"],
        autosomes_only = "--autosomes_only " if config["create_annotation_settings"]["autosomes_only"] else "",
        out = config["outputs"]["output_dir"] + "create_annotation/"
    log: config["outputs"]["output_dir"] + "log/create_gene_annotation.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --in_gtf {input.gtf} \
            --feature_name {params.feature_name} \
            {params.autosomes_only} \
            --out_dir {params.out} > {log} 2>&1
        """


rule create_snp_annotation:
    input:
        vcf = get_vcf
    output:
        annotation = config["outputs"]["output_dir"] + "create_annotation/{vcf_basename}_snpannotation.txt.gz"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["create_snp_annotation_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["create_snp_annotation_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["create_snp_annotation_time"]]
    threads: config["create_snp_annotation_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
    log: config["outputs"]["output_dir"] + "log/create_snp_annotation.{vcf_basename}.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} zcat {input.vcf} | grep "^[^#;]" | awk -v OFS='\t' '{{print $3,$1,$2}}' | gzip -c > {output.annotation}
        """


rule smf_to_gte:
    input:
        smf = config["inputs"]["gte"],
    output:
        gte = config["outputs"]["output_dir"] + "gte/gte.txt"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["smf_to_gte_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["smf_to_gte_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["smf_to_gte_time"]]
    threads: config["smf_to_gte_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
    log: config["outputs"]["output_dir"] + "log/smf_to_gte.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} awk 'BEGIN{{ FS = OFS = "\\t" }} {{ print $1, $2, "Dataset" }}' {input.smf} > {output.gte}
        """


checkpoint split_gte:
    priority: 50
    input:
        gte = get_gte,
    output:
        datasets = directory(config["outputs"]["output_dir"] + "gte/split/")
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["split_gte_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["split_gte_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["split_gte_time"]]
    threads: config["split_gte_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/split_gte.py",
        out = config["outputs"]["output_dir"] + "gte/split/"
    log: config["outputs"]["output_dir"] + "log/split_gte.log"
    shell:
        """
        mkdir -p {output.datasets}
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --gte {input.gte} \
            --out {params.out} > {log} 2>&1
        """


rule split_snpgenelimit:
    input:
        sgl = config["inputs"]["snpgenelimit"]
    output:
        vars = config["outputs"]["output_dir"] + "limit/" + get_basename(config["inputs"]["snpgenelimit"]) + "_variants.txt",
        genes = config["outputs"]["output_dir"] + "limit/" + get_basename(config["inputs"]["snpgenelimit"]) + "_genes.txt"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["split_snpgenelimit_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["split_snpgenelimit_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["split_snpgenelimit_time"]]
    threads: config["split_snpgenelimit_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        cat = "zcat" if config["inputs"]["snpgenelimit"].endswith(".gz") else "cat",
    log: config["outputs"]["output_dir"] + "log/split_snpgenelimit.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} {params.cat} {input.sgl} | awk 'BEGIN{{ FS = OFS = "\\t" }} {{ print $1 }}' > {output.vars} 
        singularity exec --bind {params.bind} {params.sif} {params.cat} {input.sgl} | awk 'BEGIN{{ FS = OFS = "\\t" }} {{ print $2 }}' > {output.genes}
        """


rule index_vcf:
    input:
        vcf = config["inputs"]["vcf"]
    output:
        index = config["inputs"]["vcf"] + ".tbi"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["index_vcf_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["index_vcf_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["index_vcf_time"]]
    threads: config["index_vcf_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
    log: config["outputs"]["output_dir"] + "log/index_vcf.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} tabix -p vcf {input.vcf}
        """


rule filter_vcf:
    input:
        vcf = config["inputs"]["vcf"],
        gte = get_gte,
        vars = get_variants
    output:
        samples = temp(config["outputs"]["output_dir"] + "genotype/samples_{limit_file}.txt"),
        vcf_gz = temp(config["outputs"]["output_dir"] + "genotype/" + get_basename(config["inputs"]["vcf"]) + "_{limit_file}.vcf.gz"),
        vcf_bgz = config["outputs"]["output_dir"] + "genotype/" + get_basename(config["inputs"]["vcf"]) + "_{limit_file}.vcf.bgz",
        index = config["outputs"]["output_dir"] + "genotype/" + get_basename(config["inputs"]["vcf"]) + "_{limit_file}.vcf.bgz.tbi"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["filter_vcf_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["filter_vcf_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["filter_vcf_time"]]
    threads: config["filter_vcf_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/filter_vcf.py"
    log: config["outputs"]["output_dir"] + "log/filter_vcf.{limit_file}.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} awk 'BEGIN{{ FS = OFS = "\\t" }} {{ print $1 }}' {input.gte} > {output.samples}
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --vcf {input.vcf} \
            --variants {input.vars} \
            --samples {output.samples} \
            --outfile {output.vcf_gz} > {log} 2>&1
            
        if [[ "$(singularity exec --bind {params.bind} {params.sif} zcat {output.vcf_gz} | grep -v "^#" | wc -l)" -eq "0" ]]; 
        then
           echo "Error, total number of SNPs in the output VCF is 0"
           rm {output.vcf_gz}
        fi
            
        singularity exec --bind {params.bind} {params.sif} gunzip -c {output.vcf_gz} | \
            singularity exec --bind {params.bind} {params.sif} bgzip > {output.vcf_bgz}
        singularity exec --bind {params.bind} {params.sif} tabix -p vcf {output.vcf_bgz}
        """


rule filter_exp:
    input:
        data = config["inputs"]["exp"],
        gte = get_dataset_gte
    output:
        samples = temp(config["outputs"]["output_dir"] + "exp/{dataset}/samples.txt"),
        out = config["outputs"]["output_dir"] + "exp/{dataset}/" + get_basename(config["inputs"]["exp"]) + ".txt.gz"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["filter_matrix_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["filter_matrix_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["filter_matrix_time"]]
    threads: config["filter_matrix_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/filter_matrix.py",
    log: config["outputs"]["output_dir"] + "log/filter_exp.{dataset}.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} awk 'BEGIN{{ FS = OFS = "\\t" }} {{ print $2 }}' {input.gte} > {output.samples}
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --data {input.data} \
            --columns {output.samples} \
            --outfile {output.out} > {log} 2>&1
        """


rule pca:
    input:
        data = get_pca_expr,
        gte = get_dataset_gte
    output:
        pca = config["outputs"]["output_dir"] + "pca/{dataset}/{infile}.Pcs.txt.gz",
        rot = config["outputs"]["output_dir"] + "pca/{dataset}/{infile}.Pcs_rot.txt.gz",
        var = config["outputs"]["output_dir"] + "pca/{dataset}/{infile}.Pcs_var.txt.gz",
        pca_plot = config["outputs"]["output_dir"] + "pca/{dataset}/{infile}.Pcs.png",
        scree_plot = config["outputs"]["output_dir"] + "pca/{dataset}/{infile}.Scree.png",
        done = config["outputs"]["output_dir"] + "pca/{dataset}/{infile}.done",
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["pca_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["pca_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["pca_time"]]
    threads: config["pca_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/pca.py",
        out = config["outputs"]["output_dir"] + "pca/{dataset}/{infile}."
    log: config["outputs"]["output_dir"] + "log/pca.{dataset}.{infile}.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --data {input.data} \
            --gte {input.gte} \
            --transpose \
            --center \
            --scale \
            --out {params.out} > {log} 2>&1
        singularity exec --bind {params.bind} {params.sif} touch {output.done}
        """


rule filter_pcs:
    input:
        data = config["outputs"]["output_dir"] + "pca/{dataset}/" + get_basename(config["inputs"]["exp"]) + ".Pcs.txt.gz"
    output:
        out = config["outputs"]["output_dir"] + "pca/{dataset}/" + get_basename(config["inputs"]["exp"]) + ".{n_pcs}Pcs.txt.gz"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["filter_matrix_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["filter_matrix_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["filter_matrix_time"]]
    threads: config["filter_matrix_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/filter_matrix.py",
    log: config["outputs"]["output_dir"] + "log/filter_pca.{dataset}.{n_pcs}Pcs.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --data {input.data} \
            --head {wildcards.n_pcs} \
            --outfile {output.out} > {log} 2>&1
        """


rule merge_covs:
    input:
        cov = config["inputs"]["cov"],
        pca = config["outputs"]["output_dir"] + "pca/{dataset}/" + get_basename(config["inputs"]["exp"]) + ".{n_pcs}Pcs.txt.gz"
    output:
        cov = config["outputs"]["output_dir"] + "cov/{dataset}/" + get_basename(config["inputs"]["exp"]) + ".cov{n_pcs}Pcs.txt.gz"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["merge_matrices_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["merge_matrices_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["merge_matrices_time"]]
    threads: config["merge_matrices_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/merge_matrices.py",
        out = lambda wildcards: config["outputs"]["output_dir"] + "cov/" + wildcards.dataset + "/" + get_basename(config["inputs"]["exp"]) + ".cov" + wildcards.n_pcs + "Pcs"
    log: config["outputs"]["output_dir"] + "log/merge_covs.{dataset}.cov{n_pcs}Pcs.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --data {input.cov} {input.pca} \
            --axis 0 \
            --out {params.out} > {log} 2>&1
        """


rule regressor:
    input:
        data = get_regressor_exp,
        cov = get_cov
    output:
        data = config["outputs"]["output_dir"] + "regressor/{dataset}/" + get_basename(config["inputs"]["exp"]) + ".{cov}.CovariatesRemovedOLS.txt.gz"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["regressor_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["regressor_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["regressor_time"]]
    threads: config["regressor_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/regressor.py",
        out = config["outputs"]["output_dir"] + "regressor/{dataset}/" + get_basename(config["inputs"]["exp"]) + ".{cov}"
    log: config["outputs"]["output_dir"] + "log/regressor.{dataset}.{cov}.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --data {input.data} \
            --cov {input.cov} \
            --out {params.out} > {log} 2>&1
        """


rule merge_exps:
    input:
        exps = lambda wildcards: expand(config["outputs"]["output_dir"] + "regressor/{dataset}/" + get_basename(config["inputs"]["exp"]) + ".{cov}.CovariatesRemovedOLS.txt.gz", dataset=get_datasets(wildcards), allow_missing=True)
    output:
        out = config["outputs"]["output_dir"] + "regressor/merged." + get_basename(config["inputs"]["exp"]) + ".{cov}.CovariatesRemovedOLS.txt.gz"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["merge_matrices_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["merge_matrices_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["merge_matrices_time"]]
    threads: config["merge_matrices_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/merge_matrices.py",
        out = lambda wildcards: config["outputs"]["output_dir"] + "regressor/merged." + get_basename(config["inputs"]["exp"]) + "." + wildcards.cov + ".CovariatesRemovedOLS"
    log: config["outputs"]["output_dir"] + "log/merge_exps.{cov}.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --data {input.exps} \
            --axis 1 \
            --out {params.out} > {log} 2>&1
        """


rule remove_gte_dataset:
    input:
        gte = get_gte
    output:
        gte = config["outputs"]["output_dir"] + "gte/gte_noDataset.txt"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["remove_gte_dataset_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["remove_gte_dataset_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["remove_gte_dataset_time"]]
    threads: config["remove_gte_dataset_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
    log: config["outputs"]["output_dir"] + "log/remove_gte_dataset.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} awk 'BEGIN{{ FS = OFS = "\\t" }} {{ print $1, $2, "Dataset" }}' {input.gte} > {output.gte}
        """


checkpoint create_batches:
    priority: 50
    input:
        genes = get_genes
    output:
        batches = directory(config["outputs"]["output_dir"] + "batches/{genes_file}/{cov}/")
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["create_batches_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["create_batches_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["create_batches_time"]]
    threads: config["create_batches_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/create_batches.py",
        n_genes = config["general_settings"]["n_genes"],
        out = config["outputs"]["output_dir"] + "batches/{genes_file}/{cov}/" + config["outputs"]["output_prefix"]
    log: config["outputs"]["output_dir"] + "log/create_batches.{genes_file}.{cov}.log"
    shell:
        """
        mkdir -p {output.batches}
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --genes {input.genes} \
            --n_genes {params.n_genes} \
            --out {params.out} > {log} 2>&1
        """


rule run_qtl:
    input:
        annotation = get_annotation,
        vcf = get_vcf,
        index = get_vcf_index,
        exp = get_exp,
        gte = get_qtl_gte
    output:
        log = config["outputs"]["output_dir"] + "output/{cov}/{batch}-log.txt.gz",
        top = config["outputs"]["output_dir"] + "output/{cov}/{batch}-TopEffects.txt",
        all = config["outputs"]["output_dir"] + "output/{cov}/{batch}-AllEffects.txt.gz" if config["qtl_settings"]["outputall"] else temp(config["outputs"]["output_dir"] + "output/{cov}/{batch}-AllEffects.txt.gz"),
        all_perm = config["outputs"]["output_dir"] + "output/{cov}/{batch}-Permutations.txt.gz" if config["qtl_settings"]["outputallpermutations"] else temp(config["outputs"]["output_dir"] + "output/{cov}/{batch}-Permutations.txt.gz"),
        snplog = config["outputs"]["output_dir"] + "output/{cov}/{batch}-snpqclog.txt.gz" if config["qtl_settings"]["snplog"] else temp(config["outputs"]["output_dir"] + "output/{cov}/{batch}-snpqclog.txt.gz"),
        finished = config["outputs"]["output_dir"] + "output/{cov}/{batch}-TopEffects.finished"
    resources:
        java_mem_gb = lambda wildcards, attempt: attempt * config["run_qtl_memory"] * config["run_qtl_threads"] - config["settings_extra"]["java_memory_buffer"],
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["run_qtl_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["run_qtl_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["run_qtl_time"]]
    threads: config["run_qtl_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        java_threads = config["run_qtl_threads"] * 2,
        jar = config["inputs"]["mbqtl_jar"],
        analysis_type = config["qtl_settings"]["analysis_type"],
        meta_analysis_method = config["qtl_settings"]["meta_analysis_method"],
        seed = config["qtl_settings"]["seed"],
        snpannotation = get_snpannotation,
        genelimit = get_genelimit,
        snplimit = get_snplimit,
        snpgenelimit = get_snpgenelimit,
        perm = config["qtl_settings"]["perm"],
        ciswindow = config["qtl_settings"]["ciswindow"],
        maf = config["qtl_settings"]["maf"],
        cr = config["qtl_settings"]["cr"],
        hwep = config["qtl_settings"]["hwep"],
        minobservations = config["qtl_settings"]["minobservations"],
        nrdatasets = config["qtl_settings"]["nrdatasets"],
        mingenotypecount = config["qtl_settings"]["mingenotypecount"],
        splitmultiallelic = "--splitmultiallelic" if config["qtl_settings"]["splitmultiallelic"] else "",
        replacemissinggenotypes = "--replacemissinggenotypes" if config["qtl_settings"]["replacemissinggenotypes"] else "",
        usehardgenotypecalls = "--usehardgenotypecalls" if config["qtl_settings"]["usehardgenotypecalls"] else "",
        norank = "--norank" if config["qtl_settings"]["norank"] else "",
        outputall = "--outputall" if config["qtl_settings"]["outputall"] else "",
        outputallpermutations = "--outputallpermutations" if config["qtl_settings"]["outputallpermutations"]  else "",
        snplog = "--snplog" if config["qtl_settings"]["snplog"] else "",
        out = config["outputs"]["output_dir"] + "output/{cov}/{batch}"
    log: config["outputs"]["output_dir"] + "log/run_qtl.{cov}.{batch}.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} java -Xmx{resources.java_mem_gb}g -Xms{resources.java_mem_gb}g \
            -Djava.util.concurrent.ForkJoinPool.common.parallelism={params.java_threads} \
            -Dmaximum.threads={params.java_threads} -Dthread.pool.size={params.java_threads} \
            -jar {params.jar} \
            --mode mbqtl \
            --{params.analysis_type} \
            --{params.meta_analysis_method} \
            --seed {params.seed} \
            --vcf {input.vcf} \
            --exp {input.exp} \
            --gte {input.gte} \
            --annotation {input.annotation} \
            {params.snpannotation} \
            {params.genelimit} \
            {params.snplimit} \
            {params.snpgenelimit} \
            --perm {params.perm} \
            --ciswindow {params.ciswindow} \
            --maf {params.maf} \
            --cr {params.cr} \
            --hwep {params.hwep} \
            --minobservations {params.minobservations} \
            --nrdatasets {params.nrdatasets} \
            --mingenotypecount {params.mingenotypecount} \
            {params.splitmultiallelic} \
            {params.replacemissinggenotypes} \
            {params.usehardgenotypecalls} \
            {params.norank} \
            {params.outputall} \
            {params.outputallpermutations} \
            {params.snplog} \
            --out {params.out} > {log} 2>&1
            
        if [[ "$(singularity exec --bind {params.bind} {params.sif} cat {output.top} | wc -l)" -le "1" ]]; 
        then
           echo "Error, no results in {output.top}"
           rm {output.top}
        fi
        
        singularity exec --bind {params.bind} {params.sif} touch {output.all}
        singularity exec --bind {params.bind} {params.sif} touch {output.all_perm}
        singularity exec --bind {params.bind} {params.sif} touch {output.snplog}
        """


rule merge:
    input:
        top = lambda wildcards: expand(config["outputs"]["output_dir"] + "output/{cov}/{batch}-TopEffects.txt", batch=get_batches(wildcards), allow_missing=True)
    output:
        top = config["outputs"]["output_dir"] + "output/{cov}/" + config["outputs"]["output_prefix"] + "-merged.txt"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["merge_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["merge_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["merge_time"]]
    threads: config["merge_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/merge.py",
        input_dir = config["outputs"]["output_dir"] + "output/{cov}/" + config["outputs"]["output_prefix"],
        out = config["outputs"]["output_dir"] + "output/{cov}/" + config["outputs"]["output_prefix"] + "-merged"
    log: config["outputs"]["output_dir"] + "log/merge.{cov}.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --input_dir {params.input_dir} \
            --out {params.out} > {log} 2>&1
        """


rule qvalues:
    input:
        top_hits = get_top_hits
    output:
        top_hits = config["outputs"]["output_dir"] + "output/{cov}/" + config["outputs"]["output_prefix"] + "-TopEffectsWithqval.txt",
        fig1 = config["outputs"]["output_dir"] + "figures/{cov}/" + config["outputs"]["output_prefix"] + "_overview.pdf",
        fig2 = config["outputs"]["output_dir"] + "figures/{cov}/" + config["outputs"]["output_prefix"] + "_hist.png"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["qvalues_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["qvalues_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["qvalues_time"]]
    threads: config["qvalues_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/multiple_testing_correction.R",
        beta_dist_a = "BetaDistAlpha",
        beta_dist_b = "BetaDistBeta",
        nom_threshold = "PvalueNominalThreshold",
        pvalue = "BetaAdjustedMetaP" if config["qtl_settings"]["perm"] > 0 else "MetaP",
        qvalue = "qval",
        alpha = config["general_settings"]["alpha"],
        data_out = config["outputs"]["output_dir"] + "output/{cov}/" + config["outputs"]["output_prefix"] + "-TopEffects",
        plot_out = config["outputs"]["output_dir"] + "figures/{cov}/" + config["outputs"]["output_prefix"],
        tmp = config["outputs"]["output_dir"] + "figures/{cov}/Rplots.pdf",
        suffix = "Withqval"
    log: config["outputs"]["output_dir"] + "log/qvalues.{cov}.log"
    shell:
        """
         singularity exec --bind {params.bind} {params.sif} Rscript {params.script} \
            --input {input.top_hits} \
            --beta_dist_a {params.beta_dist_a} \
            --beta_dist_b {params.beta_dist_b} \
            --nom_threshold {params.nom_threshold} \
            --pvalue {params.pvalue} \
            --qvalue {params.qvalue} \
            --alpha {params.alpha} \
            --data_out {params.data_out} \
            --plot_out {params.plot_out} \
            --suffix {params.suffix} > {log} 2>&1
        singularity exec --bind {params.bind} {params.sif} mv {params.tmp} {output.fig1}
        """


rule results:
    input:
        data = expand(config["outputs"]["output_dir"] + "output/{cov}/" + config["outputs"]["output_prefix"] + "-TopEffectsWithqval.txt", cov=covariates)
    output:
        out = config["outputs"]["output_dir"] + "output/" + config["outputs"]["output_prefix"] + "-results.txt"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["results_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["results_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["results_time"]]
    threads: config["results_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/results.py",
        data = config["outputs"]["output_dir"] + "output/*/" + config["outputs"]["output_prefix"] + "-TopEffectsWithqval.txt",
        nom_pvalue_column = "MetaP",
        perm_pvalue_column = "BetaAdjustedMetaP",
        qvalue_column = "qval",
        minimimal_reporting_p = config["general_settings"]["alpha"]
    log: config["outputs"]["output_dir"] + "log/results.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --data '{params.data}' \
            --nom_pvalue_column {params.nom_pvalue_column} \
            --perm_pvalue_column {params.perm_pvalue_column} \
            --qvalue_column {params.qvalue_column} \
            --minimimal_reporting_p {params.minimimal_reporting_p} \
            --out {output.out} > {log} 2>&1
        """

