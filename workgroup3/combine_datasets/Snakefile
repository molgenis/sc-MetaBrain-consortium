#!/usr/bin/env python
import json
import hashlib
import os

# Set logger level.
if config["settings_extra"]["debug"]:
    logger.set_level("DEBUG")

# Check if the input files exist.
if not os.path.exists(config["inputs"]["singularity_image"]):
    logger.error("Critical, the singularity image does not exist.\n\nExiting.")
    exit("MissingSIFFile")
if not os.path.exists(config["inputs"]["datasheet_path"]):
    logger.error("Critical, the datasheet file does not exist.\n\nExiting.")
    exit("MissingDataSheetFile")

# Add trailing /.
if not config["inputs"]["repo_dir"].endswith("/"):
    config["inputs"]["repo_dir"] += "/"
if not config["outputs"]["output_dir"].endswith("/"):
    config["outputs"]["output_dir"] += "/"

logger.info("Loading the input datasheet")
fh = open(config["inputs"]["datasheet_path"], 'r')
dataset_info = json.load(fh)
fh.close()

if "Dataset" not in dataset_info.keys():
    logger.error("\tCritical, missing 'Dataset' key in datasheet file.\n\nExiting.")
    exit("InvalidDataSheetFile")
if len(set(dataset_info["Dataset"])) != len(dataset_info["Dataset"]):
    logger.error("\tCritical, 'Dataaset' has duplicate values in datasheet file.\n\nExiting.")
    exit("InvalidDataSheetFile")
n_datasets = len(dataset_info["Dataset"])

def md5_hash(s):
    return hashlib.md5(s.encode()).hexdigest()


VCF_INPUT_FILES = {}
FILES = {}
input_files = []
for key, outfile in [("WG1-ImputedVCF", config["outputs"]["output_dir"] + "CombineVCF/imputed_hg38.vcf.gz"),
                     ("WG1-HetVCF", config["outputs"]["output_dir"] + "CombineVCF/het_filtered.vcf.gz"),
                     ("WG1-PSAM", config["outputs"]["output_dir"] + "CombineFiles/update_sex.psam"),
                     ("WG1-metadata", config["outputs"]["output_dir"] + "CombineFiles/Final_Assignments_demultiplexing_doublets.tsv.gz"),
                     ("WG2-metadata", config["outputs"]["output_dir"] + "CombineFiles/azimuth_all.metadata.tsv.gz"),
                     ("CellAnnotation", config["outputs"]["output_dir"] + "CombineFiles/cell_annotation.tsv.gz"),
                     ("PoolSheet", config["outputs"]["output_dir"] + "CombineFiles/wg0_file_directories.tsv")]:
    for file in set(dataset_info[key]):
        if file is None:
            continue
        if not os.path.exists(file):
            logger.warning("\tCritical, '{}' does not exist.\n\nExiting.".format(file))
            exit("InvalidDataSheetFile")

    if key in ["WG1-ImputedVCF", "WG1-HetVCF"]:
        for file in dataset_info[key]:
            if file is None:
                continue
            md5_hash_value = md5_hash(file)
            if md5_hash_value in VCF_INPUT_FILES:
                logger.info("\tSkipping duplicated input VCF file '{}'".format(os.path.basename(file)))
                continue
            VCF_INPUT_FILES[md5_hash_value] = file
            logger.info("\tInput VCF file '{}' is hashed as '{}'".format(os.path.basename(file), md5_hash_value))

    n_files = len(dataset_info[key])
    if n_files == 0:
        logger.warning("\tWarning, no values for key '{}'.".format(key))
        continue
    elif n_files != n_datasets:
        logger.error("\tCritical, number of files does not match number of datasets.\n\nExiting.")
        exit("InvalidDataSheetFile")
    else:
        logger.info("\tKey '{}' has {:,} values.".format(key,n_files))

    FILES[key] = {k: v for k, v in dict(zip(dataset_info["Dataset"], dataset_info[key])).items() if v is not None}
    logger.debug("\trequesting output: " + outfile)
    input_files.append(outfile)

logger.info("Valid\n")

def print_wildcards(wildcards):
    out = []
    for key, value in wildcards.items():
        out.append(key + "=" + value)
    return ", ".join(out)


def get_variants(wildcards):
    logger.debug("rule filter_vcf - get_variants({})".format(print_wildcards(wildcards)))
    vcf_hashes = [md5_hash(vcf) for _, vcf in FILES["WG1-ImputedVCF"].items()]
    if (config["inputs"]["variants_path"] is not None) and (wildcards.md5_hash in vcf_hashes):
        fpath = config["inputs"]["variants_path"]
        logger.debug("\treturn input 'variants_path': " + fpath)
        return fpath

    logger.debug("\treturning []")
    return []

def get_samples(wildcards):
    logger.debug("rule filter_vcf - get_samples({})".format(print_wildcards(wildcards)))
    if config["settings"]["filter_samples"]:
        fpath = "FilterVCF/{md5_hash}_samples.txt.gz".format(**wildcards)
        logger.debug("\treturn output of rule 'export_vcf_samples': " + fpath)
        return config["outputs"]["output_dir"] + fpath

    logger.debug("\treturning ''")
    return ""


def get_combine_vcf_input(wildcards):
    logger.debug("rule combine_vcf - get_combine_vcf_input({})".format(print_wildcards(wildcards)))
    if wildcards.vcf == "het_filtered":
        if config["settings"]["filter_samples"]:
            fpaths = [config["outputs"]["output_dir"] + "FilterVCF/{md5_hash}.vcf.bgz".format(md5_hash=md5_hash(vcf)) for _, vcf in FILES["WG1-HetVCF"].items()]
            logger.debug("\treturn filtered WG1-HetVCF input files: " + ", ".join(fpaths))
            return fpaths
        else:
            fpaths = FILES["WG1-HetVCF"].values()
            logger.debug("\treturn WG1-HetVCF input files: " + ", ".join(fpaths))
            return fpaths
    elif wildcards.vcf == "imputed_hg38":
        if config["settings"]["qc_variants"]:
            fpaths = [config["outputs"]["output_dir"] + "QualityControlVCF/{md5_hash}_qc_filtered.vcf.gz".format(md5_hash=md5_hash(vcf)) for _, vcf in FILES["WG1-ImputedVCF"].items()]
            logger.debug("\treturn QC'd WG1-ImputedVCF input files: " + ", ".join(fpaths))
            return fpaths
        elif config["inputs"]["variants_path"] is not None or config["settings"]["filter_samples"]:
            fpaths = [config["outputs"]["output_dir"] + "FilterVCF/{md5_hash}.vcf.bgz".format(md5_hash=md5_hash(vcf)) for _, vcf in FILES["WG1-ImputedVCF"].items()]
            logger.debug("\treturn filtered WG1-ImputedVCF input files: " + ", ".join(fpaths))
            return fpaths
        else:
            fpaths = FILES["WG1-ImputedVCF"].values()
            logger.debug("\treturn WG1-ImputedVCF input files: " + ", ".join(fpaths))
            return fpaths

    logger.error("Critical, Unexpected value for vcf in get_combine_vcf_input: {}\n\nExiting.".format(wildcards.vcf))
    exit()
    return []


def get_files(wildcards):
    logger.debug("rule combine_files - get_files({})".format(print_wildcards(wildcards)))
    if wildcards.outfile == "update_sex.psam":
        fpaths = FILES["WG1-PSAM"]
        logger.debug("\treturn WG1-PSAM input files: " + ", ".join(fpaths))
        return fpaths
    elif wildcards.outfile == "Final_Assignments_demultiplexing_doublets.tsv.gz":
        fpaths = FILES["WG1-metadata"]
        logger.debug("\treturn WG1-metadata input files: " + ", ".join(fpaths))
        return fpaths
    elif wildcards.outfile == "azimuth_all.metadata.tsv.gz":
        fpaths = FILES["WG2-metadata"]
        logger.debug("\treturn WG2-metadata input files: " + ", ".join(fpaths))
        return fpaths
    elif wildcards.outfile == "cell_annotation.tsv.gz":
        fpaths = FILES["CellAnnotation"]
        logger.debug("\treturn CellAnnotation input files: " + ", ".join(fpaths))
        return fpaths
    elif wildcards.outfile == "wg0_file_directories.tsv":
        fpaths = FILES["PoolSheet"]
        logger.debug("\treturn PoolSheet input files: " + ", ".join(fpaths))
        return fpaths
    else:
        logger.error("Critical, Unexpected value for outfile in get_files: {}\n\nExiting.".format(wildcards.outfile))
        exit()

    logger.debug("\treturning []")
    return []

def get_combine_files_input(wildcards):
    logger.debug("rule combine_files - get_combine_files_input({})".format(print_wildcards(wildcards)))
    fpaths = get_files(wildcards)
    datasets = sorted(fpaths.keys())
    files = [fpaths[dataset] for dataset in datasets]
    logger.debug("\treturn files: " + ", ".join(files))
    return files

def get_combine_files_datasets(wildcards):
    logger.debug("rule combine_files - get_combine_files_datasets({})".format(print_wildcards(wildcards)))
    fpaths = get_files(wildcards)
    datasets = sorted(fpaths.keys())
    logger.debug("\treturn datasets: " + ", ".join(datasets))
    return datasets


rule all:
    input:
        input_files


rule export_vcf_samples:
    input:
        wg1_metadata = FILES["WG1-metadata"].values(),
        vcf = lambda wildcards: VCF_INPUT_FILES[wildcards.md5_hash]
    output:
        samples = config["outputs"]["output_dir"] + "FilterVCF/{md5_hash}_samples.txt.gz"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["export_vcf_samples_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["export_vcf_samples_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["export_vcf_samples_time"]]
    threads: config["export_vcf_samples_threads"]
    params:
        sif = config["inputs"]["singularity_image"],
        bind = config["inputs"]["bind_path"],
        script = config["inputs"]["repo_dir"] + "scripts/export_vcf_samples.py",
    log: config["outputs"]["output_dir"] + "log/export_vcf_samples.{md5_hash}.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --wg1_metadata {input.wg1_metadata} \
            --vcf {input.vcf} \
            --outfile {output.samples} > {log} 2>&1
        """


rule filter_vcf:
    input:
        vcf = lambda wildcards: VCF_INPUT_FILES[wildcards.md5_hash],
        variants = get_variants,
        samples = get_samples
    output:
        vcf_gz = config["outputs"]["output_dir"] + "FilterVCF/{md5_hash}.vcf.gz",
        vcf_bgz = config["outputs"]["output_dir"] + "FilterVCF/{md5_hash}.vcf.bgz",
        index = config["outputs"]["output_dir"] + "FilterVCF/{md5_hash}.vcf.bgz.csi"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["filter_vcf_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["filter_vcf_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["filter_vcf_time"]]
    threads: config["filter_vcf_threads"]
    params:
        sif = config["inputs"]["singularity_image"],
        bind = config["inputs"]["bind_path"],
        script = config["inputs"]["repo_dir"] + "scripts/filter_vcf.py",
        variants = lambda wildcards: "--variants " + config["inputs"]["variants_path"] if (config["inputs"]["variants_path"] is not None) and (wildcards.md5_hash in [md5_hash(vcf) for _, vcf in FILES["WG1-ImputedVCF"].items()]) else "",
        samples = "--samples " + config["outputs"]["output_dir"] + "FilterVCF/{md5_hash}_samples.txt.gz" if config["settings"]["filter_samples"] else "",
    log: config["outputs"]["output_dir"] + "log/filter_vcf.{md5_hash}.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --vcf {input.vcf} \
            {params.variants} \
            {params.samples} \
            --outfile {output.vcf_gz} > {log} 2>&1
        singularity exec --bind {params.bind} {params.sif} gunzip -c {output.vcf_gz} | \
            singularity exec --bind {params.bind} {params.sif} bgzip > {output.vcf_bgz}
        singularity exec --bind {params.bind} {params.sif} bcftools index {output.vcf_bgz}
        """


rule qc_vcf:
    input:
        vcf = lambda wildcards: config["outputs"]["output_dir"] + "FilterVCF/{md5_hash}.vcf.bgz" if (config["inputs"]["variants_path"] is not None or config["settings"]["filter_samples"]) else VCF_INPUT_FILES[wildcards.md5_hash]
    output:
        qc_filtered = config["outputs"]["output_dir"] + "QualityControlVCF/{md5_hash}_qc_filtered.vcf.gz",
        index = config["outputs"]["output_dir"] + "QualityControlVCF/{md5_hash}_qc_filtered.vcf.gz.csi",
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["qc_vcf_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["qc_vcf_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["qc_vcf_time"]]
    threads: config["qc_vcf_threads"]
    params:
        sif = config["inputs"]["singularity_image"],
        bind = config["inputs"]["bind_path"],
        maf = config["settings"]["maf"],
        r2 = config["settings"]["r2"]
    log: config["outputs"]["output_dir"] + "log/qc_vcf.{md5_hash}.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} bcftools +fill-tags {input.vcf} -Ou -- --tags MAF | \
            singularity exec --bind {params.bind} {params.sif} bcftools filter --include 'MAF>={params.maf} & R2>={params.r2}' --write-index -Oz --output {output.qc_filtered} > {log} 2>&1
        """


rule combine_vcf:
    input:
        vcf = get_combine_vcf_input,
        indices = lambda wildcards: [fpath + ".csi" for fpath in get_combine_vcf_input(wildcards)]
    output:
        vcf = config["outputs"]["output_dir"] + "CombineVCF/{vcf}.vcf.gz",
        index1 = config["outputs"]["output_dir"] + "CombineVCF/{vcf}.vcf.gz.csi",
        index2 = config["outputs"]["output_dir"] + "CombineVCF/{vcf}.vcf.gz.tbi"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["combine_vcf_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["combine_vcf_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["combine_vcf_time"]]
    threads: config["combine_vcf_threads"]
    params:
        sif = config["inputs"]["singularity_image"],
        bind = config["inputs"]["bind_path"]
    log: config["outputs"]["output_dir"] + "log/combine_vcf.{vcf}.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} bcftools merge \
            --force-samples \
            --threads {threads} \
            {input.vcf} \
            --write-index \
            -Oz \
            --output {output.vcf} > {log} 2>&1
        singularity exec --bind {params.bind} {params.sif} tabix -p vcf {output.vcf}
        """


rule combine_files:
    input:
        files = get_combine_files_input
    output:
        outfile = config["outputs"]["output_dir"] + "CombineFiles/{outfile}"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["combine_files_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["combine_files_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["combine_files_time"]]
    threads: config["combine_files_threads"]
    params:
        sif = config["inputs"]["singularity_image"],
        bind = config["inputs"]["bind_path"],
        script = config["inputs"]["repo_dir"] + "scripts/combine_files.py",
        datasets = get_combine_files_datasets
    log: config["outputs"]["output_dir"] + "log/combine_files.{outfile}.log"
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --files {input.files} \
            --datasets {params.datasets} \
            --outfile {output.outfile} > {log} 2>&1
        """
