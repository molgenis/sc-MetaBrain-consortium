#!/usr/bin/env python
import pandas as pd
import hashlib
import os

# Set logger level.
if config["settings_extra"]["debug"]:
    logger.set_level("DEBUG")

# Check required input arguments.
if config["inputs"]["singularity_image"] is None or not os.path.exists(config["inputs"]["singularity_image"]):
    logger.error("Critical, singularity_image does not exist.\n\nExiting.")
    exit("MissingsingularityImage")
if config["inputs"]["repo_dir"] is None or not os.path.exists(config["inputs"]["repo_dir"]):
    logger.error("Critical, repo_dir does not exist.\n\nExiting.")
    exit("MissingRepoDir")
if config["inputs"]["poolsheet"] is None or not os.path.exists(config["inputs"]["poolsheet"]):
    logger.error("Critical, poolsheet file does not exist.\n\nExiting.")
    exit("MissingsingPoolSheet")
if config["inputs"]["psam"] is None or not os.path.exists(config["inputs"]["psam"]):
    logger.error("Critical, psam file does not exist.\n\nExiting.")
    exit("MissingsingPSAM")
if config["inputs"]["cell_annotation"] is None or not os.path.exists(config["inputs"]["cell_annotation"]):
    logger.error("Critical, cell annotation file does not exist.\n\nExiting.")
    exit("MissingsingCellAnnotation")
if config["inputs"]["droplet_type_annotation"] is None or not os.path.exists(config["inputs"]["droplet_type_annotation"]):
    logger.error("Critical, droplet type annotation file does not exist.\n\nExiting.")
    exit("MissingsingDropletTypeAnnotation")
if config["inputs"]["cell_type_annotation"] is None or not os.path.exists(config["inputs"]["cell_type_annotation"]):
    logger.error("Critical, cell type annotation file does not exist.\n\nExiting.")
    exit("MissingsingCellTypeAnnotation")
if config["inputs"]["rb_genes"] is None or not os.path.exists(config["inputs"]["rb_genes"]):
    logger.error("Critical, RB genes does file not exist.\n\nExiting.")
    exit("MissingsingRBGenes")
if config["inputs"]["mt_genes"] is None or not os.path.exists(config["inputs"]["mt_genes"]):
    logger.error("Critical, MT genes does file not exist.\n\nExiting.")
    exit("MissingsingMTGenes")
if config["outputs"]["output_dir"] is None:
    logger.error("Critical, the output_dir cannot be empty.\n\nExiting.")
    exit("MissingOutputDir")

# Add trailing /.
if not config["outputs"]["output_dir"].endswith("/"):
    config["outputs"]["output_dir"] += "/"

# Create the output directory.
os.makedirs(config["outputs"]["output_dir"], exist_ok=True)

# Loading poolsheet.
logger.info("Loading the input poolsheet")
POOL_DF = pd.read_csv(config["inputs"]["poolsheet"], sep="\t", dtype=str)
logger.info("\tLoaded {} with shape: {}".format(os.path.basename(config["inputs"]["poolsheet"]), POOL_DF.shape))

POOL_DF.fillna("NA", inplace=True)
POOL_DF.index = POOL_DF["Pool"]

if "Pool" not in POOL_DF.columns:
    logger.info("\tError, missing 'Pool' column in poolsheet file for the selected methods.\n\nExiting.")
    stop("InvalidPoolSheetFile")
if not POOL_DF["Pool"].is_unique:
    logger.info("\tError, your 'Pool' column contains duplicates, please make sure all values are unique.\n\nExiting.")
    stop("InvalidPoolSheetFile")

logger.info("\tValid.")
POOLS = POOL_DF["Pool"].tolist()

def get_md5_hash(s):
    return hashlib.md5(s.encode()).hexdigest()

# Define the input files.
logger.info("Loading settings from yaml:")
SETTINGS = {}
for ncount_rna in config["settings"]["ncount_rna"]:
    for nfeature_rna in config["settings"]["nfeature_rna"]:
        for percent_rb in config["settings"]["percent_rb"]:
            for percent_mt in config["settings"]["percent_mt"]:
                for malat1 in config["settings"]["malat1"]:
                    for min_cells in config["settings"]["min_cells"]:
                        settings = {
                            "nCount_RNA": ncount_rna,
                            "nFeature_RNA": nfeature_rna,
                            "percent.rb": percent_rb,
                            "percent.mt": percent_mt,
                            "MALAT1": malat1,
                            "minCells": min_cells
                        }
                        md5_hash = get_md5_hash(str(settings))
                        SETTINGS[md5_hash] = settings
                        logger.info("\t{}: {}".format(md5_hash, settings))

settings_df = pd.DataFrame(SETTINGS).T
logger.debug("\nActive settings:")
logger.debug(settings_df)

# Check if the current settings are not run before. If not, add them and update
# the settings file.
settings_fpath = config["outputs"]["output_dir"] + "settings.tsv"
if not os.path.exists(settings_fpath):
    # No settings file exists so any setting is always new.
    logger.info("\tSaving settings")
    settings_df.to_csv(settings_fpath, sep="\t", header=True, index=True)
else:
    old_settings_df = pd.read_csv(settings_fpath, sep="\t", header=0, index_col=0)
    new_settings_df = pd.concat([settings_df, old_settings_df], axis=0).drop_duplicates()

    # Skip if old settings is identical to the current settings or if the current settings
    # is already present in the old settings.
    if not old_settings_df.equals(settings_df) and not new_settings_df.equals(old_settings_df):
        logger.debug("\nPrevious settings:")
        logger.debug(old_settings_df)

        # Make sure the hashes are unique, should never happen.
        if len(set(settings_df.index)) != settings_df.shape[0]:
            logger.error("Error, duplicate hash values for unique settings.")
            exit()

        logger.debug("\nUpdated settings:")
        logger.debug(new_settings_df)

        logger.info("\tSaving updated settings")
        new_settings_df.to_csv(settings_fpath, sep="\t", header=True, index=True)

# Define the output hashes.
HASHES = SETTINGS.keys()


wildcard_constraints:
    hash = "[A-Za-z0-9]+",
    pool = "[\w-]+",
    cell_type = "[A-Za-z.]+",
    min_ind_expr = "[0-9]+",
    min_cpm = "[0-9]+"


rule all:
    input:
        exp = expand(config["outputs"]["output_dir"] + "expression/{hash}/{cell_type}.pseudobulk.{min_ind_expr}Expr{min_cpm}CPM.TMM.tsv.gz", hash=HASHES, cell_type=config["settings"]["cell_type"], min_ind_expr=config["settings"]["min_ind_expr"], min_cpm=config["settings"]["min_cpm"]),
        stats = expand(config["outputs"]["output_dir"] + "expression/{hash}/pseudobulk.sumstats.tsv.gz", hash=HASHES),
        qc_metrics = expand(config["outputs"]["output_dir"] + "figures/{hash}/ncount_rna_vs_percent_mt.png", hash=HASHES)


rule pseudobulk_pool:
    input:
        poolsheet = config["inputs"]["poolsheet"],
        psam = config["inputs"]["psam"],
        cell_annotation = config["inputs"]["cell_annotation"],
        droplet_type_annotation = config["inputs"]["droplet_type_annotation"],
        cell_type_annotation = config["inputs"]["cell_type_annotation"],
        rb_genes = config["inputs"]["rb_genes"],
        mt_genes = config["inputs"]["mt_genes"]
    output:
        metadata = config["outputs"]["output_dir"] + "expression/{hash}/pools/{pool}.metadata.tsv.gz",
        qc_metrics = config["outputs"]["output_dir"] + "expression/{hash}/pools/{pool}.qc_metrics.tsv.gz",
        full_metadata = config["outputs"]["output_dir"] + "expression/{hash}/pools/{pool}.full.metadata.tsv.gz",
        exp = config["outputs"]["output_dir"] + "expression/{hash}/pools/{pool}.pseudobulk.tsv.gz",
        stats = config["outputs"]["output_dir"] + "expression/{hash}/pools/{pool}.pseudobulk.stats.tsv.gz",
        done = config["outputs"]["output_dir"] + "expression/{hash}/pools/{pool}.done",
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["pseudobulk_pool_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["pseudobulk_pool_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["pseudobulk_pool_time"]],
    threads: config["pseudobulk_pool_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/pseudobulk.py",
        ct_pairing = "--ct_pairing " + config["inputs"]["cell_type_pairing"] if config["inputs"]["cell_type_pairing"] is not None else "",
        sample_aggregate = config["settings_extra"]["sample_aggregate"],
        ancestry = config["settings"]["ancestry"],
        cell_level = config["settings"]["cell_level"],
        ncount_rna = lambda wildcards: SETTINGS[wildcards.hash]["nCount_RNA"],
        nfeature_rna = lambda wildcards: SETTINGS[wildcards.hash]["nFeature_RNA"],
        percent_rb = lambda wildcards: SETTINGS[wildcards.hash]["percent.rb"],
        percent_mt = lambda wildcards: SETTINGS[wildcards.hash]["percent.mt"],
        malat1 = lambda wildcards: SETTINGS[wildcards.hash]["MALAT1"],
        feature_name = config["settings_extra"]["feature_name"],
        aggregate_fun = config["settings_extra"]["aggregate_fun"],
        min_cells = lambda wildcards: SETTINGS[wildcards.hash]["minCells"],
        out = config["outputs"]["output_dir"] + "expression/{hash}/pools/"
    log: config["outputs"]["output_dir"] + "log/pseudobulk_pool.{hash}.{pool}.log"
    shell:
        """
         singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --poolsheet {input.poolsheet} \
            --pool {wildcards.pool} \
            --psam {input.psam} \
            --cell_annotation {input.cell_annotation} \
            --droplet_type_annotation {input.droplet_type_annotation} \
            --cell_type_annotation {input.cell_type_annotation} \
            {params.ct_pairing} \
            --rb_genes {input.rb_genes} \
            --mt_genes {input.mt_genes} \
            --sample_aggregate {params.sample_aggregate} \
            --ancestry {params.ancestry} \
            --cell_level {params.cell_level} \
            --ncount_rna {params.ncount_rna} \
            --nfeature_rna {params.nfeature_rna} \
            --percent_rb {params.percent_rb} \
            --percent_mt {params.percent_mt} \
            --malat1 {params.malat1} \
            --feature_name {params.feature_name} \
            --min_cells {params.min_cells} \
            --aggregate_fun {params.aggregate_fun} \
            --out {params.out} > {log} 2>&1
        singularity exec --bind {params.bind} {params.sif} touch {output.exp}
        singularity exec --bind {params.bind} {params.sif} touch {output.stats}
        singularity exec --bind {params.bind} {params.sif} touch {output.done}
        """


rule merge_expr:
    input:
        poolsheet = config["inputs"]["poolsheet"],
        expr = expand(config["outputs"]["output_dir"] + "expression/{hash}/pools/{pool}.pseudobulk.tsv.gz", pool=POOLS, allow_missing=True)
    output:
        exp = config["outputs"]["output_dir"] + "expression/{hash}/{cell_type}.pseudobulk.tsv.gz",
        smf = config["outputs"]["output_dir"] + "expression/{hash}/{cell_type}.pseudobulk.smf",
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["merge_expr_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["merge_expr_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["merge_expr_time"]],
    threads: config["merge_expr_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/merge_expr.py",
        indir = config["outputs"]["output_dir"] + "expression/{hash}/pools/",
        aggregate_fun = config["settings_extra"]["aggregate_fun"],
        out = config["outputs"]["output_dir"] + "expression/{hash}/"
    log: config["outputs"]["output_dir"] + "log/merge.{hash}.{cell_type}.log"
    shell:
        """
         singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --poolsheet {input.poolsheet} \
            --indir {params.indir} \
            --cell_type {wildcards.cell_type} \
            --aggregate_fun {params.aggregate_fun} \
            --out {params.out} > {log} 2>&1
        """


rule normalise:
    input:
        exp = config["outputs"]["output_dir"] + "expression/{hash}/{cell_type}.pseudobulk.tsv.gz",
    output:
        exp = config["outputs"]["output_dir"] + "expression/{hash}/{cell_type}.pseudobulk.{min_ind_expr}Expr{min_cpm}CPM.TMM.tsv.gz",
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["normalise_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["normalise_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["normalise_time"]]
    threads: config["normalise_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/normalise.R"
    log: config["outputs"]["output_dir"] + "log/normalise.{hash}.{cell_type}.{min_ind_expr}Expr{min_cpm}CPM.log"
    shell:
        """
         singularity exec --bind {params.bind} {params.sif} Rscript {params.script} \
            --exp {input.exp} \
            --min_ind_expr {wildcards.min_ind_expr} \
            --min_cpm {wildcards.min_cpm} \
            --out {output.exp} > {log} 2>&1
        """


rule merge_stats:
    input:
        poolsheet = config["inputs"]["poolsheet"],
        stats = expand(config["outputs"]["output_dir"] + "expression/{hash}/pools/{pool}.pseudobulk.stats.tsv.gz", pool=POOLS, allow_missing=True)
    output:
        stats = config["outputs"]["output_dir"] + "expression/{hash}/pseudobulk.stats.tsv.gz",
        sumstats = config["outputs"]["output_dir"] + "expression/{hash}/pseudobulk.sumstats.tsv.gz"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["merge_expr_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["merge_expr_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["merge_expr_time"]],
    threads: config["merge_expr_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/merge_stats.py",
        indir = config["outputs"]["output_dir"] + "expression/{hash}/pools/",
        out = config["outputs"]["output_dir"] + "expression/{hash}/"
    log: config["outputs"]["output_dir"] + "log/merge.{hash}.log"
    shell:
        """
         singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --poolsheet {input.poolsheet} \
            --indir {params.indir} \
            --out {params.out} > {log} 2>&1
        """


rule visualise_barcode_qc:
    input:
        poolsheet = config["inputs"]["poolsheet"],
        qc_metrics = expand(config["outputs"]["output_dir"] + "expression/{hash}/pools/{pool}.full.metadata.tsv.gz", pool=POOLS, allow_missing=True)
    output:
        qc_metrics = config["outputs"]["output_dir"] + "expression/{hash}/qc_metrics.tsv.gz",
        smf = config["outputs"]["output_dir"] + "figures/{hash}/ncount_rna_vs_percent_mt.png",
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["visualise_barcode_qc_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["visualise_barcode_qc_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["visualise_barcode_qc_time"]],
    threads: config["visualise_barcode_qc_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/visualise_barcode_qc.py",
        indir = config["outputs"]["output_dir"] + "expression/{hash}/pools/",
        cell_level = config["settings"]["cell_level"],
        ncount_rna = lambda wildcards: SETTINGS[wildcards.hash]["nCount_RNA"],
        nfeature_rna = lambda wildcards: SETTINGS[wildcards.hash]["nFeature_RNA"],
        percent_rb = lambda wildcards: SETTINGS[wildcards.hash]["percent.rb"],
        percent_mt = lambda wildcards: SETTINGS[wildcards.hash]["percent.mt"],
        malat1 = lambda wildcards: SETTINGS[wildcards.hash]["MALAT1"],
        palette = "--palette " + config["inputs"]["palette"] if config["inputs"]["palette"] is not None else "",
        data_out = config["outputs"]["output_dir"] + "expression/{hash}/",
        plot_out = config["outputs"]["output_dir"] + "figures/{hash}/"
    log: config["outputs"]["output_dir"] + "log/visualise_barcode_qc.{hash}.log"
    shell:
        """
         singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --poolsheet {input.poolsheet} \
            --indir {params.indir} \
            --cell_level {params.cell_level} \
            --ncount_rna {params.ncount_rna} \
            --nfeature_rna {params.nfeature_rna} \
            --percent_rb {params.percent_rb} \
            --percent_mt {params.percent_mt} \
            --malat1 {params.malat1} \
            {params.palette} \
            --data_out {params.data_out} \
            --plot_out {params.plot_out} > {log} 2>&1
        """

