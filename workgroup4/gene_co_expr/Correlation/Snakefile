#!/usr/bin/env python
import pandas as pd
import os

# Set logger level.
if config["settings_extra"]["debug"]:
    logger.set_level("DEBUG")

# Check required input arguments.
if config["inputs"]["singularity_image"] is None or not os.path.exists(config["inputs"]["singularity_image"]):
    logger.error("Critical, singularity_image does not exist.\n\nExiting.")
    exit("MissingSingularityImage")
if config["inputs"]["repo_dir"] is None or not os.path.exists(config["inputs"]["repo_dir"]):
    logger.error("Critical, repo_dir does not exist.\n\nExiting.")
    exit("MissingRepoDir")
if config["inputs"]["poolsheet"] is None or not os.path.exists(config["inputs"]["poolsheet"]):
    logger.error("Critical, poolsheet file does not exist.\n\nExiting.")
    exit("MissingPoolSheet")
if config["inputs"]["psam"] is None or not os.path.exists(config["inputs"]["psam"]):
    logger.error("Critical, psam file does not exist.\n\nExiting.")
    exit("MissingPSAM")
if config["inputs"]["cell_annotation"] is None or not os.path.exists(config["inputs"]["cell_annotation"]):
    logger.error("Critical, cell annotation file does not exist.\n\nExiting.")
    exit("MissingCellAnnotation")
if config["inputs"]["droplet_type_annotation"] is None or not os.path.exists(config["inputs"]["droplet_type_annotation"]):
    logger.error("Critical, droplet type annotation file does not exist.\n\nExiting.")
    exit("MissingDropletTypeAnnotation")
if config["inputs"]["cell_type_annotation"] is None or not os.path.exists(config["inputs"]["cell_type_annotation"]):
    logger.error("Critical, cell type annotation file does not exist.\n\nExiting.")
    exit("MissingCellTypeAnnotation")
if config["inputs"]["gene_annotation"] is None or not os.path.exists(config["inputs"]["gene_annotation"]):
    logger.error("Critical, gene annotation file does not exist.\n\nExiting.")
    exit("MissingGeneAnnotation")
if config["inputs"]["rb_genes"] is None or not os.path.exists(config["inputs"]["rb_genes"]):
    logger.error("Critical, RB genes file does not exist.\n\nExiting.")
    exit("MissingRBGenes")
if config["inputs"]["mt_genes"] is None or not os.path.exists(config["inputs"]["mt_genes"]):
    logger.error("Critical, MT genes file does not exist.\n\nExiting.")
    exit("MissingMTGenes")
if config["outputs"]["output_dir"] is None:
    logger.error("Critical, the output_dir cannot be empty.\n\nExiting.")
    exit("MissingOutputDir")

# Add trailing /.
if not config["outputs"]["output_dir"].endswith("/"):
    config["outputs"]["output_dir"] += "/"

# Checking settings with options to be valid.
if config["settings"]["weight"] not in ["sumcounts", "nonzero"]:
    logger.error("Critical, weight must be 'sumcounts' or 'nonzero'.\n\nExiting.")
    exit("InvalidWeight")
if config["settings_extra"]["feature_name"] not in ["HGNC", "ENSG", "HGNC_ENSG"]:
    logger.error("Critical, feature_name must be 'HGNC', 'ENSG', or 'HGNC_ENSG'.\n\nExiting.")
    exit("InvalidFeatureName")

# Define the chromosomes.
if config["settings"]["chromosomes"] is None:
  config["settings"]["chromosomes"] = [str(i) for i in range(1, 23)]

# Create the output directory.
os.makedirs(config["outputs"]["output_dir"], exist_ok=True)

# Loading poolsheet.
logger.info("Loading the input poolsheet")
POOL_DF = pd.read_csv(config["inputs"]["poolsheet"], sep="\t", dtype=str)
logger.info("\tLoaded {} with shape: {}".format(os.path.basename(config["inputs"]["poolsheet"]), POOL_DF.shape))

POOL_DF.fillna("NA", inplace=True)
POOL_DF.index = POOL_DF["Pool"]

if "Pool" not in POOL_DF.columns:
    logger.info("\tError, missing 'Pool' column in poolsheet file for the selected methods.\n\nExiting.")
    stop("InvalidPoolSheetFile")
if not POOL_DF["Pool"].is_unique:
    logger.info("\tError, your 'Pool' column contains duplicates, please make sure all values are unique.\n\nExiting.")
    stop("InvalidPoolSheetFile")

logger.info("\tValid.")
POOLS = POOL_DF["Pool"].tolist()

wildcard_constraints:
    pool = "[\w-]+",
    sample = "[\w-]+",
    cell_type = "[A-Za-z.]+"

def print_wildcards(wildcards):
    out = []
    for key, value in wildcards.items():
        out.append(key + "=" + value)
    return ", ".join(out)

def get_pool_samples(wildcards):
    logger.debug("get_pool_samples({})".format(print_wildcards(wildcards)))
    pool_samples = {"pool": [], "sample": []}
    n_samples = 0
    for pool in POOLS:
        # Per pool, look which samples it contained for the current cell type.
        out_dir = checkpoints.filter_split_counts.get(**wildcards, pool=pool).output["data_dir"]
        glob_fpath = os.path.join(out_dir, pool + ".{sample}." + wildcards.cell_type + ".raw.counts.h5")
        logger.debug("\tglob path: " + glob_fpath)
        samples = glob_wildcards(glob_fpath).sample
        logger.debug("\tpool = {} has samples = {}".format(pool, ", ".join(samples)))
        if len(samples) == 0:
            continue
        pool_samples["pool"].extend([pool] * len(samples))
        pool_samples["sample"].extend(samples)
        n_samples += len(samples)

    logger.debug("\tfound '{:,}' samples".format(n_samples))
    return pool_samples


rule all:
    input:
        corr = expand(config["outputs"]["output_dir"] + "correlations/log/{cell_type}/{cell_type}.chr.{chr}.corr.done",cell_type=config["settings"]["cell_type"],chr=config["settings"]["chromosomes"])
        # aggregated_corr = expand(config["outputs"]["combined_result_dir"] + "correlations/data/{cell_type}/chr{chr}/{cell_type}.chr.{chr}.corr.transpose.stripped.txt.gz",cell_type=config["settings"]["cell_type"],chr=config["settings"]["chromosomes"])


checkpoint filter_split_counts:
    input:
        poolsheet = config["inputs"]["poolsheet"],
        psam = config["inputs"]["psam"],
        cell_annotation = config["inputs"]["cell_annotation"],
        droplet_type_annotation = config["inputs"]["droplet_type_annotation"],
        cell_type_annotation = config["inputs"]["cell_type_annotation"],
        rb_genes = config["inputs"]["rb_genes"],
        mt_genes = config["inputs"]["mt_genes"]
    output:
        full_metadata = config["outputs"]["output_dir"] + "expression/{pool}/{pool}.full.metadata.tsv.gz",
        # sumcounts_weights = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.sumcounts.weights.txt.gz",
        # nonzero_weights = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.nonzero.weights.txt.gz",
        # counts = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.raw.counts.h5",
        data_dir = directory(config["outputs"]["output_dir"] + "expression/{pool}/data/"),
        stats = config["outputs"]["output_dir"] + "expression/{pool}/{pool}.pseudobulk.stats.tsv.gz",
        done = config["outputs"]["output_dir"] + "expression/{pool}/{pool}.done",
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["filter_split_counts_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["filter_split_counts_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["filter_split_counts_time"]],
    threads: config["filter_split_counts_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/filter_split_counts.py",
        ct_pairing = lambda wildcards: "--ct_pairing " + config["inputs"]["cell_type_pairing"] if config["inputs"]["cell_type_pairing"] is not None else "",
        sample_aggregate = config["settings_extra"]["sample_aggregate"],
        ancestry = config["settings"]["ancestry"],
        cell_level = config["settings"]["cell_level"],
        ncount_rna = config["barcode_qc_settings"]["ncount_rna"],
        nfeature_rna = config["barcode_qc_settings"]["nfeature_rna"],
        complexity = config["barcode_qc_settings"]["complexity"],
        percent_rb = config["barcode_qc_settings"]["percent_rb"],
        percent_mt = config["barcode_qc_settings"]["percent_mt"],
        malat1 = config["barcode_qc_settings"]["malat1"],
        cap_barcodes = lambda wildcards: "--cap_barcodes " + str(config["barcode_qc_settings"]["cap_barcodes"]) if config["barcode_qc_settings"]["cap_barcodes"] is not None else "",
        cr_barcodes = lambda wildcards: "--cr_barcodes" if config["barcode_qc_settings"]["cellranger_barcodes"] else "",
        min_cells = config["barcode_qc_settings"]["min_cells"],
        feature_name = config["settings_extra"]["feature_name"],
        out = config["outputs"]["output_dir"] + "expression/{pool}/"
    log: config["outputs"]["output_dir"] + "log/filter_split_counts.{pool}.log"
    shell:
        """
         singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --poolsheet {input.poolsheet} \
            --pool {wildcards.pool} \
            --psam {input.psam} \
            --cell_annotation {input.cell_annotation} \
            --droplet_type_annotation {input.droplet_type_annotation} \
            --cell_type_annotation {input.cell_type_annotation} \
            {params.ct_pairing} \
            --feature_name {params.feature_name} \
            --rb_genes {input.rb_genes} \
            --mt_genes {input.mt_genes} \
            --sample_aggregate {params.sample_aggregate} \
            --ancestry {params.ancestry} \
            --cell_level {params.cell_level} \
            --ncount_rna {params.ncount_rna} \
            --nfeature_rna {params.nfeature_rna} \
            --complexity {params.complexity} \
            --percent_rb {params.percent_rb} \
            --percent_mt {params.percent_mt} \
            --malat1 {params.malat1} \
            {params.cap_barcodes} \
            {params.cr_barcodes} \
            --min_cells {params.min_cells} \
            --out {params.out} > {log} 2>&1 
        singularity exec --bind {params.bind} {params.sif} touch {output.stats} {output.done}
        """


# Important to check if filter_split_counts is done to prevent snakemake errors.
rule average_read_counts:
    input:
        fsc_done = expand(config["outputs"]["output_dir"] + "expression/{pool}/{pool}.done", pool=POOLS, allow_missing=True),
        weights = lambda wildcards: expand(config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.sumcounts.weights.txt.gz", zip, **get_pool_samples(wildcards), allow_missing=True)
    output:
        avg_read_depth = config["outputs"]["output_dir"] + "expression/data/{cell_type}/sumcounts.avg_read_depth.json"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["average_read_counts_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["average_read_counts_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["average_read_counts_time"]],
    threads: config["average_read_counts_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/average_read_counts.py",
        feature_name = config["settings_extra"]["feature_name"],
        min_obs_per_gene = config["correlate_settings"]["min_obs_per_gene"],
        chunk_size = config["correlate_settings"]["chunk_size"],
    log: config["outputs"]["output_dir"] + "log/average_read_counts.{cell_type}.sumcounts.log"
    shell:
        """
         singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --read_counts {input.weights} \
            --outfile {output.avg_read_depth} > {log} 2>&1
        """


rule proportional_fit_and_log:
    input:
        counts = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.raw.counts.h5",
        avg_rd = config["outputs"]["output_dir"] + "expression/data/{cell_type}/sumcounts.avg_read_depth.json"
    output:
        out = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.pflog1.counts.h5",
        weights = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.pflog1.weights.txt.gz"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["proportional_fit_and_log_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["proportional_fit_and_log_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["proportional_fit_and_log_time"]],
    threads: config["proportional_fit_and_log_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/normalise.py",
        out = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.pflog1pf."
    log: config["outputs"]["output_dir"] + "log/proportional_fit_and_log.{cell_type}.{pool}.{sample}.log"
    shell:
        """
         singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --counts {input.counts} \
            --avg_rd {input.avg_rd} \
            --log1p \
            --weights_out {output.weights} \
            --out {output.out} > {log} 2>&1
        """


# Important to check if filter_split_counts is done to prevent snakemake errors.
rule average_read_counts_pflogpf:
    input:
        fsc_done = expand(config["outputs"]["output_dir"] + "expression/{pool}/{pool}.done", pool=POOLS, allow_missing=True),
        weights = lambda wildcards: expand(config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.pflog1.weights.txt.gz", zip, **get_pool_samples(wildcards), allow_missing=True)
    output:
        avg_read_depth = config["outputs"]["output_dir"] + "expression/data/{cell_type}/pflog1.avg_read_depth.json"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["average_read_counts_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["average_read_counts_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["average_read_counts_time"]],
    threads: config["average_read_counts_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/average_read_counts.py",
        feature_name = config["settings_extra"]["feature_name"],
        min_obs_per_gene = config["correlate_settings"]["min_obs_per_gene"],
        chunk_size = config["correlate_settings"]["chunk_size"],
    log: config["outputs"]["output_dir"] + "log/average_read_counts.{cell_type}.pflog1.log"
    shell:
        """
         singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --read_counts {input.weights} \
            --outfile {output.avg_read_depth} > {log} 2>&1
        """


rule proportional_fit:
    input:
        counts = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.pflog1.counts.h5",
        avg_rd = config["outputs"]["output_dir"] + "expression/data/{cell_type}/pflog1.avg_read_depth.json"
    output:
        out = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.pflog1pf.counts.h5",
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["proportional_fit_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["proportional_fit_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["proportional_fit_time"]],
    threads: config["proportional_fit_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/normalise.py",
        out = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.pflog1pf."
    log: config["outputs"]["output_dir"] + "log/proportional_fit_and_log.{cell_type}.{pool}.{sample}.log"
    shell:
        """
         singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --counts {input.counts} \
            --avg_rd {input.avg_rd} \
            --out {output.out} > {log} 2>&1
        """


rule correlate_genes:
    input:
        counts = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.pflog1pf.counts.h5",
        weights = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}." + config["settings"]["weight"] + ".weights.txt.gz"
    output:
        correlation = config["outputs"]["output_dir"] + "correlations/data/{cell_type}/chr{chr}/{pool}.{sample}.chr.{chr}.corr.dat"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["correlate_genes_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["correlate_genes_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["correlate_genes_time"]],
    threads: config["correlate_genes_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/correlate_genes.py",
        gene_annotation = config["inputs"]["gene_annotation"],
        min_obs_per_gene = config["correlate_settings"]["min_obs_per_gene"],
        chunk_size = config["correlate_settings"]["chunk_size"],
        out = config["outputs"]["output_dir"] + "correlations/data/{cell_type}/chr{chr}/{pool}.{sample}",
        egene_list = lambda wildcards: "--egenelist " + config["inputs"]["e_gene_list"][wildcards.cell_type] if wildcards.cell_type in config["inputs"]["e_gene_list"] else "",
        coegene_list = lambda wildcards: "--coegenelist " + config["inputs"]["co_egene_list"][wildcards.cell_type] if wildcards.cell_type in config["inputs"]["co_egene_list"] else "",
    log: config["outputs"]["output_dir"] + "log/{cell_type}/chr{chr}/{pool}.{sample}.chr.{chr}.log"
    shell:
        """
         singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --counts {input.counts} \
            --weights {input.weights} \
            --geneannotation {params.gene_annotation} \
            --chr {wildcards.chr} \
            --min_obs {params.min_obs_per_gene} \
            --chunk_size {params.chunk_size} \
            {params.egene_list} \
            {params.coegene_list} \
            --binary \
            --out {params.out} > {log} 2>&1
        """


rule correlation_check:
    input: 
        fsc_done = expand(config["outputs"]["output_dir"] + "expression/{pool}/{pool}.done", pool=POOLS),
        correlation = lambda wildcards: expand(config["outputs"]["output_dir"] + "correlations/data/{cell_type}/chr{chr}/{pool}.{sample}.chr.{chr}.corr.dat", zip, **get_pool_samples(wildcards), allow_missing=True)
    output:
        done = config["outputs"]["output_dir"] + "correlations/log/{cell_type}/{cell_type}.chr.{chr}.corr.done"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["correlation_check_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["correlation_check_memory"],
        time = lambda wildcards,attempt: config["cluster_time"][(attempt - 1) + config["correlation_check_time"]]
    threads: config["correlation_check_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"]
    shell:
        """
        singularity exec --bind {params.bind} {params.sif} touch {output.done}
        """
