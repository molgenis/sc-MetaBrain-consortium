#!/usr/bin/env python
import pandas as pd
import os

# Set logger level.
if config["settings_extra"]["debug"]:
    logger.set_level("DEBUG")

# Check required input arguments.
if config["inputs"]["singularity_image"] is None or not os.path.exists(config["inputs"]["singularity_image"]):
    logger.error("Critical, singularity_image does not exist.\n\nExiting.")
    exit("MissingsingularityImage")
if config["inputs"]["repo_dir"] is None or not os.path.exists(config["inputs"]["repo_dir"]):
    logger.error("Critical, repo_dir does not exist.\n\nExiting.")
    exit("MissingRepoDir")
if config["inputs"]["poolsheet"] is None or not os.path.exists(config["inputs"]["poolsheet"]):
    logger.error("Critical, poolsheet file does not exist.\n\nExiting.")
    exit("MissingsingPoolSheet")
if config["inputs"]["psam"] is None or not os.path.exists(config["inputs"]["psam"]):
    logger.error("Critical, psam file does not exist.\n\nExiting.")
    exit("MissingsingPSAM")
if config["inputs"]["cell_annotation"] is None or not os.path.exists(config["inputs"]["cell_annotation"]):
    logger.error("Critical, cell annotation file does not exist.\n\nExiting.")
    exit("MissingsingCellAnnotation")
if config["inputs"]["droplet_type_annotation"] is None or not os.path.exists(config["inputs"]["droplet_type_annotation"]):
    logger.error("Critical, droplet type annotation file does not exist.\n\nExiting.")
    exit("MissingsingDropletTypeAnnotation")
if config["inputs"]["cell_type_annotation"] is None or not os.path.exists(config["inputs"]["cell_type_annotation"]):
    logger.error("Critical, cell type annotation file does not exist.\n\nExiting.")
    exit("MissingsingCellTypeAnnotation")
if config["inputs"]["rb_genes"] is None or not os.path.exists(config["inputs"]["rb_genes"]):
    logger.error("Critical, RB genes does file not exist.\n\nExiting.")
    exit("MissingsingRBGenes")
if config["inputs"]["mt_genes"] is None or not os.path.exists(config["inputs"]["mt_genes"]):
    logger.error("Critical, MT genes does file not exist.\n\nExiting.")
    exit("MissingsingMTGenes")
if config["inputs"]["gtf"] is None or not os.path.exists(config["inputs"]["gtf"]):
    logger.error("Critical, GTF genes does file not exist.\n\nExiting.")
    exit("MissingsingGTF")
if config["outputs"]["output_dir"] is None:
    logger.error("Critical, the output_dir cannot be empty.\n\nExiting.")
    exit("MissingOutputDir")

# Add trailing /.
if not config["outputs"]["output_dir"].endswith("/"):
    config["outputs"]["output_dir"] += "/"

# Create the output directory.
os.makedirs(config["outputs"]["output_dir"], exist_ok=True)

# Loading poolsheet.
logger.info("Loading the input poolsheet")
POOL_DF = pd.read_csv(config["inputs"]["poolsheet"], sep="\t", dtype=str)
logger.info("\tLoaded {} with shape: {}".format(os.path.basename(config["inputs"]["poolsheet"]), POOL_DF.shape))

POOL_DF.fillna("NA", inplace=True)
POOL_DF.index = POOL_DF["Pool"]

if "Pool" not in POOL_DF.columns:
    logger.info("\tError, missing 'Pool' column in poolsheet file for the selected methods.\n\nExiting.")
    stop("InvalidPoolSheetFile")
if not POOL_DF["Pool"].is_unique:
    logger.info("\tError, your 'Pool' column contains duplicates, please make sure all values are unique.\n\nExiting.")
    stop("InvalidPoolSheetFile")

logger.info("\tValid.")
POOLS = POOL_DF["Pool"].tolist()

wildcard_constraints:
    pool = "[\w-]+",
    sample = "[\w-]+",
    cell_type = "[A-Za-z.]+"


def print_wildcards(wildcards):
    out = []
    for key, value in wildcards.items():
        out.append(key + "=" + value)
    return ", ".join(out)

def get_pool_samples(wildcards, cell_type):
    logger.debug("get_pool_samples({}, cell_type=)".format(print_wildcards(wildcards)))
    pool_samples = {"pool": [], "sample": []}
    n_samples = 0
    for pool in POOLS:
        # Per pool, look which samples it contained for the current cell type.
        out_dir = checkpoints.filter_split_counts.get(**wildcards, pool=pool).output["data_dir"]
        glob_fpath = os.path.join(out_dir, pool + ".{sample}." + cell_type + ".raw.counts.h5")
        logger.debug("\tglob path: " + glob_fpath)
        samples = glob_wildcards(glob_fpath).sample
        logger.debug("\tpool = {} has samples = {}".format(pool, ", ".join(samples)))
        if len(samples) == 0:
            continue
        pool_samples["pool"].extend([pool] * len(samples))
        pool_samples["sample"].extend(samples)
        n_samples += len(samples)

    logger.debug("\tfound '{:,}' samples".format(n_samples))
    return pool_samples


def get_input_all(wildcards):
    logger.debug("get_input_all({})".format(print_wildcards(wildcards)))
    input_files = []
    for cell_type in config["settings"]["cell_type"]:
        for chr in config["settings"]["chromosomes"]:
            pool_samples = get_pool_samples(wildcards, cell_type=cell_type)
            for pool, sample in zip(pool_samples["pool"], pool_samples["sample"]):
                input_files.append(config["outputs"]["output_dir"] + "correlations/data/log/{cell_type}/{pool}.{sample}.chr.{chr}.corr.done".format(cell_type=cell_type, pool=pool, sample=sample,chr=chr))
    logger.debug("\requesting '{:,}' input files".format(len(input_files)))
    return input_files

def get_samples(wildcards):
    logger.debug("get_samples({})".format(print_wildcards(wildcards)))
    samples = []
    for cell_type in config["settings"]["cell_type"]:
        pool_samples = get_pool_samples(wildcards, cell_type=cell_type)
        for sample in pool_samples["sample"]:
            samples.append(sample)
    return samples

rule all:
    input:
        gene_correlations = get_input_all


checkpoint filter_split_counts:
    input:
        poolsheet = config["inputs"]["poolsheet"],
        psam = config["inputs"]["psam"],
        cell_annotation = config["inputs"]["cell_annotation"],
        droplet_type_annotation = config["inputs"]["droplet_type_annotation"],
        cell_type_annotation = config["inputs"]["cell_type_annotation"],
        rb_genes = config["inputs"]["rb_genes"],
        mt_genes = config["inputs"]["mt_genes"]
    output:
        full_metadata = config["outputs"]["output_dir"] + "expression/{pool}/{pool}.full.metadata.tsv.gz",
        # weights = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.raw.weights.txt.gz",
        # counts = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.raw.counts.h5",
        data_dir = directory(config["outputs"]["output_dir"] + "expression/{pool}/data/"),
        stats = config["outputs"]["output_dir"] + "expression/{pool}/{pool}.pseudobulk.stats.tsv.gz",
        done = config["outputs"]["output_dir"] + "expression/{pool}/{pool}.done",
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["filter_split_counts_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["filter_split_counts_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["filter_split_counts_time"]],
    threads: config["filter_split_counts_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/filter_split_counts.py",
        ct_pairing = lambda wildcards: "--ct_pairing " + config["inputs"]["cell_type_pairing"] if config["inputs"]["cell_type_pairing"] is not None else "",
        sample_aggregate = config["settings_extra"]["sample_aggregate"],
        ancestry = config["settings"]["ancestry"],
        cell_level = config["settings"]["cell_level"],
        ncount_rna = config["barcode_qc_settings"]["ncount_rna"],
        nfeature_rna = config["barcode_qc_settings"]["nfeature_rna"],
        complexity = config["barcode_qc_settings"]["complexity"],
        percent_rb = config["barcode_qc_settings"]["percent_rb"],
        percent_mt = config["barcode_qc_settings"]["percent_mt"],
        malat1 = config["barcode_qc_settings"]["malat1"],
        cap_barcodes = lambda wildcards: "--cap_barcodes " + str(config["barcode_qc_settings"]["cap_barcodes"]) if config["barcode_qc_settings"]["cap_barcodes"] is not None else "",
        cr_barcodes = lambda wildcards: "--cr_barcodes" if config["barcode_qc_settings"]["cellranger_barcodes"] else "",
        min_cells = config["barcode_qc_settings"]["min_cells"],
        feature_name = config["settings_extra"]["feature_name"],
        out = config["outputs"]["output_dir"] + "expression/{pool}/"
    log: config["outputs"]["output_dir"] + "log/filter_split_counts.{pool}.log"
    shell:
        """
         singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --poolsheet {input.poolsheet} \
            --pool {wildcards.pool} \
            --psam {input.psam} \
            --cell_annotation {input.cell_annotation} \
            --droplet_type_annotation {input.droplet_type_annotation} \
            --cell_type_annotation {input.cell_type_annotation} \
            {params.ct_pairing} \
            --feature_name {params.feature_name} \
            --rb_genes {input.rb_genes} \
            --mt_genes {input.mt_genes} \
            --sample_aggregate {params.sample_aggregate} \
            --ancestry {params.ancestry} \
            --cell_level {params.cell_level} \
            --ncount_rna {params.ncount_rna} \
            --nfeature_rna {params.nfeature_rna} \
            --complexity {params.complexity} \
            --percent_rb {params.percent_rb} \
            --percent_mt {params.percent_mt} \
            --malat1 {params.malat1} \
            {params.cap_barcodes} \
            {params.cr_barcodes} \
            --min_cells {params.min_cells} \
            --out {params.out} > {log} 2>&1
        singularity exec --bind {params.bind} {params.sif} touch {output.stats} {output.done}
        """


# Important to check if merge_sample_counts is done to prevent snakemake errors.
rule average_read_counts:
    input:
        fsc_done = expand(config["outputs"]["output_dir"] + "expression/{pool}/{pool}.done",pool=POOLS,allow_missing=True),
        weights = lambda wildcards: expand(config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.{dtype}.weights.txt.gz", pool=POOLS, sample = get_samples(wildcards),allow_missing=True)
    output:
        avg_read_depth = config["outputs"]["output_dir"] + "expression/data/{cell_type}/{dtype}.avg_read_depth.json"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["average_read_counts_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["average_read_counts_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["average_read_counts_time"]],
    threads: config["average_read_counts_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/average_read_counts.py",
        feature_name = config["settings_extra"]["feature_name"],
        min_obs_per_gene = config["correlate_settings"]["min_obs_per_gene"],
        chunk_size = config["correlate_settings"]["chunk_size"],
    log: config["outputs"]["output_dir"] + "log/average_read_counts.{cell_type}.{dtype}.log"
    shell:
        """
         singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --read_counts {input.weights} \
            --outfile {output.avg_read_depth} > {log} 2>&1
        """


rule proportional_fit_and_log:
    input:
        counts = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.raw.counts.h5",
        avg_rd = config["outputs"]["output_dir"] + "expression/data/{cell_type}/raw.avg_read_depth.json"
    output:
        out = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.pflog1p.counts.h5",
        weights = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.pflog1p.weights.txt.gz"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["proportional_fit_and_log_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["proportional_fit_and_log_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["proportional_fit_and_log_time"]],
    threads: config["proportional_fit_and_log_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/normalise.py",
        out = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.pflog1p."
    log: config["outputs"]["output_dir"] + "log/proportional_fit_and_log.{cell_type}.{pool}.{sample}.log"
    shell:
        """
         singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --counts {input.counts} \
            --avg_rd {input.avg_rd} \
            --log1p \
            --weights_out {output.weights} \
            --out {output.out} > {log} 2>&1
        """


rule proportional_fit:
    input:
        counts = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.pflog1p.counts.h5",
        avg_rd = config["outputs"]["output_dir"] + "expression/data/{cell_type}/pflog1p.avg_read_depth.json"
    output:
        out = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.pflog1ppf.counts.h5",
        # weights = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.pflog1ppf.weights.txt.gz"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["proportional_fit_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["proportional_fit_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["proportional_fit_time"]],
    threads: config["proportional_fit_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/normalise.py",
        out = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.pflog1ppf."
    log: config["outputs"]["output_dir"] + "log/proportional_fit_and_log.{cell_type}.{pool}.{sample}.log"
    shell:
        """
         singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --counts {input.counts} \
            --avg_rd {input.avg_rd} \
            --out {output.out} > {log} 2>&1
        """


rule correlate_genes:
    input:
        counts = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.pflog1ppf.counts.h5",
        weights = config["outputs"]["output_dir"] + "expression/{pool}/data/{pool}.{sample}.{cell_type}.raw.weights.txt.gz",
    output:
        done = config["outputs"]["output_dir"] + "correlations/data/log/{cell_type}/{pool}.{sample}.chr.{chr}.corr.done"
    resources:
        mem_per_thread_gb = lambda wildcards, attempt: attempt * config["correlate_genes_memory"],
        disk_per_thread_gb = lambda wildcards, attempt: attempt * config["correlate_genes_memory"],
        time = lambda wildcards, attempt: config["cluster_time"][(attempt - 1) + config["correlate_genes_time"]],
    threads: config["correlate_genes_threads"]
    params:
        bind = config["inputs"]["bind_path"],
        sif = config["inputs"]["singularity_image"],
        script = config["inputs"]["repo_dir"] + "scripts/correlate_genes.py",
        gene_annotation = config["inputs"]["gene_annotation"],
        min_obs_per_gene = config["correlate_settings"]["min_obs_per_gene"],
        chunk_size = config["correlate_settings"]["chunk_size"],
        out = config["outputs"]["output_dir"] + "correlations/data/{cell_type}/{pool}.{sample}",
        gene_list = config["inputs"]["e_gene_list"]
    log: config["outputs"]["output_dir"] + "log/correlate_genes.{cell_type}/{pool}.{sample}.chr.{chr}.log"
    shell:
        """
         singularity exec --bind {params.bind} {params.sif} python {params.script} \
            --counts {input.counts} \
            --weights {input.weights} \
            --geneannotation {params.gene_annotation} \
            --chr {wildcards.chr} \
            --min_obs {params.min_obs_per_gene} \
            --chunk_size {params.chunk_size} \
            --binary \
            --out {params.out} > {log} 2>&1
        singularity exec --bind {params.bind} {params.sif} touch {output.done}
        """